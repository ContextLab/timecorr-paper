\documentclass[english]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{apacite}
\usepackage{hyperref}
\usepackage[sort]{natbib}
\usepackage{pxfonts}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[left]{lineno}
\usepackage{soul}
\linenumbers


\title{High-level cognition is supported by at least second order
  dynamic correlations in neural activity patterns}

\author{Lucy L. W. Owen$^1$, Thomas H. Chang$^{1,2}$, and\
  Jeremy R. Manning\textsuperscript{$1, \dagger$}\\
  [0.1in]$^1$Department of Psychological and Brain
  Sciences,\\Dartmouth
  College, Hanover, NH\\
  $^3$Amazon.com, Seattle, WA\\
  \textsuperscript{$\dagger$}Address correspondence to
  jeremy.r.manning@dartmouth.edu}


\begin{document}
\maketitle


\begin{abstract}
  Our thoughts arise from coordinated patterns of interactions between
  brain structures that change with our ongoing experiences.
  High-order dynamic correlations in brain activity patterns reflect
  different subgraphs of the brain's connectome that display
  homologous lower-level dynamic correlations.  We tested the
  hypothesis that high-level cognition is supported by high-order
  dynamic correlations in brain activity patterns.  We developed an
  approach to estimating high-order dynamic correlations in timeseries
  data, and we applied the approach to neuroimaging data collected as
  human participants either listened to a ten-minute story or a
  temporally scrambled version of the story, or underwent a resting
  state scan.  We trained across-participants pattern classifiers to
  decode (in held-out data) when in the session each activity snapshot
  was collected.  We found that classifiers trained to decode from
  high-order dynamic correlations yielded better performance on data
  collected as participants listened to the (unscrambled) story.  By
  contrast, classifiers trained to decode data from scrambled versions
  of the story or during the resting state scan yielded the best
  performance when they were trained using first-order dynamic
  correlations or raw activity patterns.  We suggest that as our
  thoughts become more complex, they are supported by higher-order
  patterns of dynamic network interactions throughout the brain.
\end{abstract}

\doublespacing

\section*{Introduction}
A central goal in cognitive neuroscience is to elucidate the
\textit{neural code}: the mapping between (a) mental states or
cognitive representations and (b) neural activity patterns. One means
of testing models of the neural code is to ask how accurately that
model is able to ``translate'' neural activity patterns into known (or
hypothesized) mental states or cognitive
representations~\citep[e.g.,][]{HaxbEtal01, NormEtal06, TongPrat12,
  MitcEtal08a, KamiTong05, NishEtal11, PereEtal18, HuthEtal12,
  HuthEtal16}.  Training decoding models on different types of neural
features can also help to elucidate which specific aspects of neural
activity patterns are informative about cognition-- and, by extension,
which types of neural activity patterns might comprise the neural
code.  For example, prior work has used region of interest analyses to
estimate the anatomical locations of specific neural
representations~\citep[e.g.,][]{EtzeEtal09}, or to compare the
relative contributions to the neural code of multivariate activity
patterns versus patterns of dynamic correlations between neural
activity patterns~\citep[e.g.,][]{MannEtal18, FongEtal19}.  An
emerging theme in this literature is that cognition is mediated by
complex dynamic interactions between brain
structures~\citep{SporHone06, BassEtal06, Turk13, DemeEtal19}.

Studies of the neural code to date have primarily focused on
univariate or multivariate neural patterns~\cite[for review
see][NormEtal06], or (more recently) on patterns of dynamic
first-order correlations~\citep[i.e., interactions between pairs of
brain structures;][]{MannEtal18, FongEtal19}.  We wondered what the
future of this line of work might hold.  For example, is the neural
code mediated by higher-order interactions between brain structures?
Second-order correlations reflect \textit{homologous} patterns of
correlation.  In other words, if the changing patterns of correlations
between two regions, $A$ and $B$, are similar to those between two
other regions, $C$ and $D$, this would be reflected in the
second-order correlations between ($A$--$B$) and ($C$--$D$).  In this
way, second-order correlations identify similarities and differences
between subgraphs of the brain's connectome.  Analogously, third-order
correlations reflect homologies between second-order correlations--
i.e., homologous patterns of homologous interactions between brain
regions.  More generally, higher-order correlations reflect homologies
between patterns of lower-order correlations.  We can then ask: which
``orders'' of interaction are most reflective of high-level cognitive
processes?

Another central question pertains to the extent to which the neural
code is carried by activity patterns that directly reflect ongoing
cognition~\citep[e.g., following][]{HaxbEtal01, NormEtal06}, versus
the dynamic properties of the network structure itself, independent of
specific activity patterns in any given set of regions~\citep[e.g.,
following][]{BassEtal06}.  For example, graph theoretic measures such
as centrality and degree~\citep{BullSpor09} may be used to estimate
how a given brain structure is ``communicating'' with other
structures, independently of the specific neural representations
carried by those structures.  If one considers a brain region's graph
theoretic position in the network (e.g., its eigenvector centrality)
as a dynamic property, one can compare how the positions of different
regions are correlated, and/or how those patterns of correlations change
over time.  We can also compute higher-order patterns in these
correlations to characterize homologous subgraphs in the connectome
that display similar changes in their constituent brain structures'
interactions with the rest of the brain.

To gain insights into the above aspects of the neural code, we
developed a computational framework for estimating dynamic high-order
correlations in timeseries data. This framework provides an important
advance, in that it enables us to examine patterns in higher-order
correlations that are computationally intractable to estimate via
conventional methods.  Given a multivariate timeseries, our framework
provides timepoint-by-timepoint estimates of the first-order
correlations, second-order correlations, and so on (up to tenth-order
correlations in this manuscript).  Our approach combines a
kernel-based method for computing dynamic correlations in timeseries
data with a dimensionality reduction step that projects the resulting
dynamic correlations into a low-dimensional space.  We explored two
dimensionality reduction approaches: principle components
analysis~\citep[PCA;][]{Pear01}, which preserves an approximately invertable
transformation back to the original data; and a second non-invertible
algorithm that explored patterns in eigenvector
centrality~\citep{Land95}.  This latter approach characterizes
correlations between each feature dimension's relative
\textit{position} in the network in favor of the specific activity
histories of different features.

We validated our approach using synthetic data where the underlying
correlations were known.  We then applied our framework to a
neuroimaging dataset collected as participants
listened to either an audio recording of a ten-minute story or a
temporally scrambled version of the story, or underwent a resting
state scan~\citep{SimoEtal16}.  We used a subset of the data to train
across-participant classifiers to decode listening times using a blend
of neural features (comprising neural activity patterns, as well as
different orders of correlations between those patterns that were
inferred using our computational framework).  We found that both the
PCA-based and eigenvector centrality-based approaches yielded neural
patterns that could be used to decode accurately.  Both approaches
also yielded the best decoding accuracy for data collected during
(intact) story listening when high-order (PCA: second-order;
eigenvector centrality: fourth-order) dynamic correlation patterns
were included as features.  When we trained classifiers on the
scambled stories or resting state data, only lower-order dynamic
patterns were informative to the decoders.  Taken together, our
results indicate that high-level cognition is supported by high-order
dynamic patterns of communication between brain structures.

\section*{Methods}
Our general approach to comprises four general steps
(Fig.~\ref{fig:methods}).  First, we derive a kernel-based approach to
computing dynamic pairwise correlations in a $T$ (timepoints) by $K$
(features) multivariate timeseries, $\mathbf{X}_0$.  This yields a $T$
by $\mathcal{O}(K^2)$ matrix of dynamic correlations, $\mathbf{Y}_1$,
where each row comprises the upper triangle of the correlation matrix
at a single timepoint, reshaped into a row vector (this reshaped
vector is $(\frac{K^2 - K}{2})$-dimensional).  Second, we apply a
dimensionality reduction step to project the matrix of dynamic
correlations back onto a $K$-dimensional space.  This yields a $T$ by
$K$ matrix, $\mathbf{X}_1$, that reflects an approximation of the
dynamic correlations reflected in the original data.  Third, we use
repeated applications of the kernel-based dynamic correlation step to
$\mathbf{X}_n$ and the dimensionality reduction step to the resulting
$\mathbf{Y}_{n+1}$ to estimate high-order dynamnic correlations.  Each
application of these steps to a $T$ by $K$ time series $\mathbf{X}_n$
yields a $T$ by $K$ matrix, $\mathbf{X}_{n+1}$, that reflects the
dynamic correlations between the columns of $\mathbf{X}_n$.  In this
way, we refer to $n$ as the \textit{order} of the timeseries, where
$\mathbf{X}_0$ (order 0) denotes the original data and $\mathbf{X}_n$
denotes $n^\mathrm{th}$-order dynamic correlations between the columns
of $\mathbf{X}_0$.  Finally, we use a cross-validation--based decoding
approach to evaluate how well information contained in a given order
(or weighted mixture of orders) may be used to decode relevant
cognitive states.  If including a given $\mathbf{X}_n$ in the feature
set yields higher classification accuracy on held-out data, we
interpret this as evidence that the given cognitive states are
reflected in patterns of $n^\mathrm{th}$-order correlations.  All of
the code used to produce the figures and results in this manuscript,
along with links to the corresponding datasets, may be found at
\href{https://github.com/ContextLab/timecorr-paper}{github.com/ContextLab/timecorr-paper}.  In addition, we have
released a Python toolbox for computing dynamic high-order
correlations in timeseries data; our toolbox may be found at
\href{https://timecorr.readthedocs.io/}{timecorr.readthedocs.io}. \textbf{JRM NOTE: CHECK LINK}

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{figs/methods_fig}
  \caption{\textbf{Estimating dynamnic high-order correlations.}  Given
    a $T$ by $K$ matrix of multivariate timeseries data,
    $\mathbf{Y}_n$ (where $n \in \mathbb{N}, n \geq 0$), we use
    Equation~\ref{eqn:timecorr} to compute a timeseries of
    $K$ by $K$ correlation matrices, $\mathbf{Y}_{n+1}$.  We then
    approximate $\mathbf{Y}_{n+1}$ with the $T$ by $K$ matrix
    $\mathbf{X}_{n+1}$.  This process may be repeated to scalably estimate
    iteratively higher-order correlations in the data.  Note that the
    transposes of $\mathbf{Y}_n$ and $\mathbf{X}_{n+1}$ are displayed
    in the figure for compactness.
  \label{fig:methods}}
\end{figure}


\subsection*{Kernel-based approach for computing dynamic correlations}
Given a matrix of observations, we can compute the (static)
Pearson's correlation between any pair of columns, $\mathbf{X}(\cdot, i)$ and
$\mathbf{X}(\cdot, j)$ using~\citep{Pear01}:
\begin{align}
  \mathrm{corr}(\mathbf{X}(\cdot, i), \mathbf{X}(\cdot, j)) &=
                                                              \frac{\sum_{t=1}^T
                                                              \left(\mathbf{X}(t,
                                                              i)
                                                              -
                                                              \bar{\mathbf{X}(\cdot,
                                                              i)}\right)
                                                              \left(\mathbf{X}(t,
                                                              j)
                                                              -
                                                              \bar{\mathbf{X}(\cdot, j)}\right)}{\sqrt{\sum_{t=1}^T
                                                              \sigma^2_{\mathbf{X}(\cdot, i)} 
                                                              \sigma^2_{\mathbf{X}(\cdot, j)}}},~\mathrm{where}\\\label{eqn:corr}
  \bar{\mathbf{X}(\cdot, k)} &= \sum_{t=1}^T
                       \mathbf{X}(t, k),~\mathrm{and}\\
  \sigma^2_{\mathbf{X}(\cdot, k)} &= \sum_{t=1}^T \left( \mathbf{X}(t, k) -
                            \bar{\mathbf{X}(\cdot, k)} \right)^2 
\end{align}
We can generalize this formula to compute time-varying correlations by
incorporating a \textit{kernel function} that takes a time $t$ as
input, and returns how much the observed data at each timepoint
$\tau \in \left[ -\infty, \infty \right]$ contributes to the estimated instantaneous
correlation at time $t$ (Fig.~\ref{fig:kernels}).
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/kernels}
  \caption{\textbf{Examples of kernel functions.} Each panel
    displays per-timepoint weights at $t = 50$, evaluated for 100
    timepoints ($\tau \in \left[1, ..., 100\right]$).  \textbf{a. Uniform
      kernel.} The weights are timepoint-invariant; observations at
    all timepoints are weighted equally, and do not change as a
    function of $\tau$.  This is a special case of weight function that
    reduces dynamic correlations to static correlations.
    \textbf{b. Dirac $\delta$ kernel.} Only the observation at
    timepoint $t$ is given a non-zero weight (of 1).
    \textbf{c. Gaussian kernels.} Each kernel's weights fall off
    in time according to a Gaussian probability density function
    centered on time $t$.  Weights derived using several different
    example variance parameters ($\sigma^2$ are displayed.
    \textbf{d. Laplace kernels.}  Each kernel's weights fall off
    in time according to a Laplace probability density function
    centered on time $t$.  Weights derived using several different
    example scale parameters ($b$) are displayed.  \textbf{e. Mexican
      hat (Ricker wavelet) kernels.}  Each kernel's weights fall
    off in time according to a Ricker wavelet centered on time $t$.  This
    function highlights the \textit{contrasts} between local versus
    surrounding activity patterns in estimating dynamic
    correlations. Weights derived using several different example
    width parameters ($\sigma$) are displayed.}
  \label{fig:kernels}
\end{figure}

Given a kernel function $\kappa_t(\cdot)$ for timepoint $t$,
evaluated at timepoints $\tau \in \left[ 1, ..., T \right]$, we
can update the static correlation formula in Equation~\ref{eqn:corr}
to estimate the \textit{instantaneous correlation} at timepoint $t$:
\begin{align}
  \mathrm{timecorr}_{\kappa_t}\left(\mathbf{X}(\cdot, i), \mathbf{X}(\cdot, j)\right) &= \frac{\sum_{\tau=1}^T \left( \mathbf{X}(\tau, i) -
                                       \widetilde{\mathbf{X}}_{\kappa_t}(i) \right)
                                 \left( \mathbf{X}(\tau, j) -
                                        \widetilde{\mathbf{X}}_{\kappa_t}(j)\right)}
              {\sqrt{\sum_{\tau=1}^T
                                              \widetilde{\sigma}_{\kappa_t}^2(\mathbf{X}(\cdot,
                                                                                        i))
                                              \widetilde{\sigma}_{\kappa_t}^2(\mathbf{X}(\cdot, j))}},~\mathrm{where}\\\label{eqn:timecorr}
  \widetilde{\mathbf{X}}_{\kappa_t}(t, k) &= \sum_{\tau=1}^T
                       \kappa_t(\tau, k)\mathbf{X}(\tau, k),\\
  \widetilde{\sigma}_{\kappa_t}^2(\mathbf{X}(\cdot, k)) &= \sum_{\tau=1}^T
                                                  \left(
                                                  \mathbf{X}(\tau, k) -
                            \widetilde{\mathbf{X}}_{\kappa_t}(t, k) \right)^2.
\end{align}
Here $\mathrm{timecorr}(\mathbf{X}(\cdot, i), \mathbf{X}(\cdot, j),
  \kappa_t)$ reflects the correlation at time $t$ between columns $i$
  and $j$ of $\mathbf{X}$, estimated using the kernel $\kappa_t$.

\subsubsection*{Dynamic inter-subject functional connectivity (DISFC)}
Equation~\ref{eqn:timecorr} provides a means of taking a single
observation matrix, $\mathbf{X}_n$ and estimating the dynamic
correlations from moment to moment, $\mathbf{Y}_{n+1}$.  Suppose that
one has access to a set of multiple observation matrices that reflect
the same phenomenon.  For example, one might collect neuroimaging data
from several experimental participants, as each participant performs
the same task (or sequence of tasks).  Let $\mathbf{X}_n^1$,
$\mathbf{X}_n^2$, ..., $\mathbf{X}_n^P$ reflect the $T$ by $K$
observation matrices ($n = 0$) or reduced correlation matrices
($n > 0$) for each of $P$ participants in an experiment.  We can use
\textit{inter-subject functional connectivity}~\citep[ISFC;
][]{SimoEtal16} to compute the degree of stimulus-driven correlations
reflected in the multi-participant dataset at a given timepoint $t$
using:
\begin{align}
\bar{\mathbf{C}}(t) = M\left(R\left(\frac{1}{2P} \sum_{p=1}^P
  Z\left(Y_n^p(t)\right)^\mathrm{T} + Z\left(Y_n^p(t)\right)\right)\right),\label{eqn:disfc}
\end{align}
where $M$ extracts and vectorizes the diagonal and upper triangle of a symmetric
matrix, $Z$ is the Fisher $z$-transformation~\citep{Zar10}:
\begin{align}
Z(r) = \frac{\log(1+r) - \log(1-r)}{2}
\end{align}
$R$ is the inverse of $Z$:
\begin{align}
R(z) = \frac{\exp(2z - 1)}{\exp(2z + 1)},
\end{align}
and $\mathbf{Y}_n^p(t)$ denotes the correlation matrix
(Eqn.~\ref{eqn:corr}) between each column of $\mathbf{X}_n^p$ and each
column of the average $\mathbf{X}_n$ from all \textit{other}
participants, $\bar{\mathbf{X}}_n^{ \backslash p}$:
\begin{align}
  \bar{\mathbf{X}}_n^{ \backslash p} = R\left(\frac{1}{P-1}\sum_{q \in
  \backslash p} Z\left( \mathbf{X}_n^q \right) \right),
\end{align}
where $\backslash p$ denotes the set of all participants other than
participant $p$. In this way, the $T$ by $\frac{K^2 - K}{2}$ DISFC
matrix $\bar{\mathbf{C}}$ provides a time-varying extension of the ISFC
approach developed by \cite{SimoEtal16}.

\subsection*{Low-dimensional representations of dynamic
  correlations}

Given a $T$ by $\frac{K^2 - K}{2}$ matrix of dynamic correlations,
$\mathbf{Y}_n$, we propose two general approaches to computing a $T$
by $K$ low-dimensional representation of these correlations,
$\mathbf{X}_n$.  The first approach uses dimensionality reduction
algorithms to project $\mathbf{Y}_n$ onto a $K$-dimensional space.
The second approach uses graph-theoretic measures to characterize the
relative positions of each feature ($k \in \left[1, ..., K \right]$)
in the network defined by the correlation matrix at each timepoint.

\subsubsection*{Dimensionality reduction-based approaches to computing
  $\mathbf{X}_n$}

The modern library of dimensionality reduction algorithms include
Principal Components Analysis~\citep[PCA; ][]{Pear01}, Probabilistic
PCA~\citep[PPCA; ][]{TippBish99}, Exploratory Factor
Analysis~\citep[EFA; ][]{Spea04}, Independent Components
Analysis~\citep[ICA; ][]{JuttHera91, ComoEtal91}, $t$-Stochastic
Neighbor Embedding~\citep[$t$-SNE; ][]{MaatHint08}, Uniform Manifold
Approximation and Projection~\citep[UMAP; ][]{McInHeal18},
non-negative matrix factorization~\citep[NMF; ][]{LeeSeun99},
Topographic Factor Analysis (TFA)~\cite{MannEtal14b}, Hierarchical
Topographic Factor analysis (HTFA)~\cite{MannEtal18}, Topographic
Latent Source Analysis (TLSA)~\cite{GersEtal11}, Dictionary
learning~\citep{MairEtal09a, MairEtal09b}, deep
autoencoders~\citep{HintSala06}, among others.  While complete
characterizations of each of these algorithms is beyond the scope of
the present manuscript, the general intuition driving these approaches
is to compute the $T$ by $I$ matrix, $\mathbf{X}$, that is closest
to the original $T$ by $J$ matrix, $\mathbf{Y}$, where (typically)
$I \ll J$.  The different approaches place different constraints on
what properties $\mathbf{X}$ must satisfy and which aspects of
the data are compared (and how) to characterize the match between
$\mathbf{X}$ and $\mathbf{Y}$.

Applying dimensionality reduction algorithms to $\mathbf{Y}$ yields a
$\mathbf{X}$ whose columns reflect weighted combinations (or nonlinear
transformations) of the original columns of $\mathbf{Y}$.  This has
two main consequences.  First, with each repeated dimensionality
reduction, the resulting $\mathbf{X}_n$ has lower and lower fidelity
(with respect to what the ``true'' $\mathbf{Y}_n$ might have looked
like without using dimensionality reduction to maintain scalability).
In other words, computing $\mathbf{X}_n$ is a lossy operation.
Second, whereas each columns of $\mathbf{Y}_n$ may always be mapped
directly onto specific pairs of columns of $\mathbf{Y}_{n-1}$, the
columns of $\mathbf{X}_n$ reflect weighted combinations and/or
nonlinear transformations of the columns of $\mathbf{Y}_n$.  Many
dimensionality reduction algorithms are invertable (or approximately
invertable).  However, attempting to map a given $\mathbf{X}_n$ back
onto the original feature space of $\mathbf{Y}_0$ will usually require
$\mathcal{O}(TK^{2n})$ space and therefore quickly becomes intractable
as $n$ or $K$ grow large.

\subsubsection*{Graph theoretic approaches to computing
  $\mathbf{X}_n$}
The above dimensionality reduction approaches to approximating a given
$\mathbf{Y}_n$ with a lower-dimensional $\mathbf{X}_n$ preserve a
(potentially recombined and transformed) mapping back to the original
data in $\mathbf{Y}_0$.  We also explore graph theoretic approaches
that forgo a preserved mapping back to the original data in favor of
preserving each feature's relative \textit{position} in the broader
network of interactions and connections.  To illustrate the
distinction between the two general approaches we explore, suppose a
network comprises nodes $A$, $B$, and $C$.  If $A$ and $B$ exhibit
uncorrelated activity patterns, the functional connection between
them will be (by definition) close to 0.  However, if $A$ and $B$ each
interact with $C$ in similar ways, we might attempt to capture those
similarities using a measure that reflects the how $A$ and $B$
interact in the network.

In general, graph theoretic measures take as input a matrix of
interactions (e.g., using the above notation, an $K$ by $K$
correlation matrix or binarized correlation matrix reconstituded from
a single timepoint's row of $\mathbf{Y}$) and return as output a set
of $K$ measures describing how each node (feature) sits within that
correlation matrix with respect to the rest of the population.  Widely
used measures include betweeness centrality~\citep[the proportion of
shortest paths between each pair of nodes in the population that
involves the given node in question; e.g., ][]{Newm05, OpsaEtal10,
  Bart04, GeisEtal08, Free77}; diversity and
dissimilarity~\citep[characterizations of how differently connected a
given node is from others in the population; e.g., ][]{Rao82, Lin09,
  RicoSzei06}; Eigenvector centrality and pagerank
centrality~\citep[measures of how influential a given node is within
the broader network; e.g., ][]{Newm08, Bona07, LohmEtal10,
  HaluEtal13}; transfer entropy and flow coefficients~\citep[a measure
of how much information is flowing from a given node to other nodes in
the network; e.g., ][]{HoneEtal07, Schr00}; $k$-coreness
centrality~\citep[a measure of the connectivity of a node within its
local sub-graph; e.g., ][]{AlvaEtal05, ChriFowl10}; within-module
degree~\citep[a measure of how many connections a node has to its
close neighbors in the network; e.g., ][]{RubiSpor10}; participation
coefficient~\citep[a measure of the diversity of a node's connections
to different sub-graphs in the network; e.g., ][]{RubiSpor10}; and
sub-graph centrality~\citep[a measure of a node's participation in all
of the network's sub-graphs; e.g., ][]{EstrRodr05}; among others.

For a given graph theoretic measure,
$\eta: \mathbb{R}^{K \times K} \rightarrow \mathbb{R}^K$, we can use
$\eta$ to tranform each row of $\mathbf{Y}_n$ in a way that
characterizes the corresponding graph-theoretic properties of each
column.  This results in a new $T$ by $K$ matrix, $\mathbf{X}_n$, that
reflects how the features reflected in the columns of $\mathbf{Y}_n$
participate in the network during each timepoint (row).


\subsection*{Higher-order correlations}

Because $\mathbf{X}_n$ has the same shape as the original data
$\mathbf{X}_0$, approximating $\mathbf{Y}_n$ with a lower-dimensional
$\mathbf{X}_n$ enables us to estimate high-order dynamic correlations
in a scalable way.  Given a $T$ by $K$ input matrix, the output of
Equation~\ref{eqn:timecorr} requires $\mathcal{O}(TK^2)$ space to
store.  Repeated applications of Equation~\ref{eqn:timecorr} (i.e.,
computing dynamic correlations between the columns of the outputted
dynamic correlation matrix) each require exponentially more space; in
general the $n^\mathrm{th}$-order dynamic correlations of a $T$ by $K$
timeseries occupies $\mathcal{O}(TK^{2n})$ space.  However, when we
approximate or summarize the output of Equation~\ref{eqn:timecorr} with a $T$ by
$K$ matrix (as described above), it becomes feasible to compute even
very high-order correlations in high-dimensional data.  Specifically,
approximating the $n^\mathrm{th}$-order dynamic correlations of a $T$
by $K$ timeseries requires only $\mathcal{O}(TK^2)$ additional space--
the same as would be required to compute first-order dynamic
correlations. In other words, the space required to store $n+1$
multivariate timeseries reflecting up to $n^\mathrm{th}$ order
correlations in the original data scales linearly with $n$ using our
approach (Fig.~\ref{fig:methods}).

\subsection*{Data}
We examined two types of data: synthetic data and human functional
neuroimaging data.  We constructed and leveraged the synthetic data to
evaluate our general approach.  Specifically, we tested how well
Equation~\ref{eqn:timecorr} could be used to recover known dynamic
correlations using different choices of kernel ($\kappa$;
Fig.~\ref{fig:kernels}), for each of several synthetic datasets that
exhibited different temporal properties.  We applied our approach to a
functional neuroimaging dataset to test the hypothesis that ongoing
cognitive processing is reflected in high-order dynamic correlations.
We used an across-participant classification test to estimate whether
dynamic correlations of different orders contain information about
which timepoint in a story participants were listening to.

\subsubsection*{Synthetic data}
We constructed a total of 40 multivariate timeseries, collectively
reflecting a total of 4 different patterns of dynamic correlations
(i.e., 10 datasets reflecting each type of dynamic pattern).  Each
timeseries comprised 50 features (dimensions) that varied over 300
timepoints.  The observations at each timepoint were drawn from a
zero-mean multivariate Gaussian distribution with a covariance matrix
defined for each timepoint as described below.  We drew the observations at
each timepoint independently from the draws at all other timepoints;
in other words, for each observation $s_t \sim
\mathcal{N}\left(\mathbf{0}, \mathbf{\Sigma}_t\right)$ at timepoint $t$, $p(s_t) = p(s_t | p_{\backslash t})$.

\paragraph*{Constant.}  We generated data with stable underlying
correlations to evaluate how Equation~\ref{eqn:timecorr} characterized
correlation ``dynamics'' when the ground truth correlations were
static.  We constructed 10 multivariate timeseries, whose observations
were each drawn from a single (stable) Gaussian distribution.  For
each dataset, we constructed a random covariance matrix,
$\mathbf{\Sigma}_m$:
\begin{align}
  \mathbf{C}(i, j) &\sim \mathcal{N}(0, 1)\\
  \mathbf{\Sigma}_m &= \mathbf{C}
                                          \mathbf{C}^\top~\mathrm{, where}\label{eqn:randcorr}
  \end{align}
  $i, j \in \left[1, 2, ..., 50 \right]$.  In other words, all of the
  observations (for each of the 300 timepoints) within each dataset
  were drawn from a multivariate Gaussian distribution with the same
  covariance matrix, and the 10 datasets each used a different
  covariance matrix.
  
  \paragraph*{Random.}  We generated a second set of 10 synthetic
  datasets whose observations at each timepoint were drawn from a
  Gaussian distribution with a new randomly constructed (using
  Eqn.~\ref{eqn:randcorr}) covariance matrix.  Because each
  timepoint's covariance matrix was drawn independently of the
  covariance matrices for all other timepoints, these datasets
  provided a test of reconstruction accuracy in the absence of any
  meaningful underlying temporal structure in the dynamic correlations
  underlying the data.
  
  \paragraph*{Ramping.}  We generated a third set of 10 synthetic
  datasets whose underlying correlations changed gradually over time.
  For each dataset, we constructed two \textit{anchor} correlation
  matrices using Equation~\ref{eqn:randcorr},
  $\mathbf{\Sigma}_{\mathrm{start}}$ and
  $\mathbf{\Sigma}_{\mathrm{end}}$.  For each of the 300 timepoints in
  each dataset, we drew the observations from a multivariate Gaussian
  distribution whose covariance matrix at each timepoint $t \in
  \left[0, ..., 299\right]$ was given by
  \begin{align}
    \mathbf{\Sigma}_t = \left( 1 - \frac{1 - t}{299} \right)
    \mathbf{\Sigma}_{\mathrm{start}} + \frac{t}{299}\mathbf{\Sigma}_{\mathrm{end}}.
  \end{align}
The gradually changing correlations underlying these datasets allow us
to evaluate the recovery of dynamic correlations when each timepoint's
correlation matrix is unique (as in the random datasets), but where
the correlation dynamics are structured.
  
\paragraph*{Event.} We generated a fourth set of 10 synthetic datasets
whose underlying correlation matrices exhibited prolonged intervals of
stability, intersperced with abrupt changes.  For each dataset, we
used Equation~\ref{fig:randcorr} to generate 5 random covariance
matrices.  We constructed a timeseries where each set of 60 consecutive samples
was drawn from a Gaussian with the same covariance matrix.  These
datasets were intended to simulate a system that undergoes occasional
abrupt state changes.

\subsubsection*{Functional neuroimaging data collected during story
  listening}
We examined an fMRI dataset collected by \cite{SimoEtal16} that the
authors have made publically available at
\href{http://arks.princeton.edu/ark:/88435/dsp015d86p269k}{arks.princeton.edu/ark:/88435/dsp015d86p269k}.  The dataset
comprises neuroimaging data collected as participants listened
to an audio recording of a story (intact condition; 36 participants),
listened to time scrambled recordings of the same story (17
participants in the paragraph-scrambled condition listened to the
paragraphs in a randomized order and 36 in the word-scrambled
condition listened to the words in a randomized order), or lay resting
with their eyes open in the scanner (rest condition; 36
participants).  Full neuroimaging details may be found in the original
paper for which the data were collected~\citep{SimoEtal16}.

\paragraph{Hierarchical topographic factor analysis (HTFA).}
Following our prior related work, we used HTFA~\citep{MannEtal18} to
derive a compact representation of the data.  In brief, this approach
approximates the timeseries of voxel activations (44,415 voxels) using
a much smaller number of radial basis function nodes (in this case 700
nodes).  This provides a convenient representation for examining
full-brain network dynamics.  All of the analyses we carried out on
the neuroimaging dataset were performed in this lower-dimensional
space.  In other words, each participant's data matrix,
$\mathbf{Y}_0$, was a number-of-timepoints by 700 matrix of
HTFA-derived factor weights (where the row and column labels were
matched across participants).  Code for carrying out HTFA on fMRI data
may be found as part of the BrainIAK toolbox~\citep{brainiak}, which
may be downloaded at \href{https://brainiak.org/}{brainiak.org}.

\subsection*{Temporal decoding}
We sought to identify neural patterns that reflected participants'
ongoing cognitive processing of incoming stimulus information.  As
reviewed by \cite{SimoEtal16}, one way of homing in on these
stimulus-driven neural patterns is to compare activity patterns across
individuals (e.g., using ISFC analyses).  In particular, neural
patterns will be similar across individuals, to the extent that the
neural patterns under consideration are stimulus driven, and to the
extent that the corresponding cognitive representations are reflected
in similar spatial patterns across people.  Following this logic, we
used an across-participants temporal decoding test developed by
\cite{MannEtal18} to assess the degree to which different neural
patterns reflected ongoing stimulus-driven cognitive processing across
people.  The approach entails using a subset of the data to train a
classifier to decode which stimulus timepoint (i.e., moment in the
story participants listened to).  We use decoding (forward inference)
accuracy on held-out data, from held-out participants, as a proxy for
the extent to which the inputted neural patterns reflected
stimulus-driven cognitive processing in a similar way across
individuals.

\subsubsection*{Forward inference and decoding accuracy}
We used an across-participants correlation-based classifier to decode
which stimulus timepoint matched a given neural pattern.  We first
divided the participants into two groups: a template group,
$\mathcal{G}_{\mathrm{template}}$, and a to-be-decoded group,
$\mathcal{G}_{\mathrm{decode}}$.  We used Equation~\ref{eqn:disfc} to
compute a DISFC matrix for each group
($\bar{\textbf{C}}_{\mathrm{template}}$ and
$\bar{\textbf{C}}_{\mathrm{decode}}$, respectively).  We then
correlated the rows of $\bar{\textbf{C}}_{\mathrm{template}}$ and
$\bar{\textbf{C}}_{\mathrm{decode}}$ to form a number-of-timepoints by
number-of-timepoints decoding matrix, $\mathbf{\Lambda}$.  In this
way, the rows of $\mathbf{\Lambda}$ reflected timepoints from the
template group, while the columns reflected timepoints from the
to-be-decoded group.  We assigned temporal labels to each row
$\bar{\textbf{C}}_{\mathrm{decode}}$ using the row of
$\bar{\textbf{C}}_{\mathrm{template}}$ to which it was most highly
correlated.  We then repeated this decoding procedure, but using
$\mathcal{G}_{\mathrm{decode}}$ as the template group and
$\mathcal{G}_{\mathrm{template}}$ as the to-be-decoded group.  Given
the true timepoint labels (for each group), we defined the
\textit{decoding accuracy} as the proportion of correctly decoded
timepoints, across both groups.

\subsubsection*{Feature weighting and testing}
We sought to examine which types of neural features (i.e.,
activations, first-order dynamic correlations, and higher-order
dynamic correlations) were informative to the temporal decoders.
Using the notation above, these features correspond to $\mathbf{Y}_0$,
$\mathbf{X}_1$, $\mathbf{X}_2$, $\mathbf{X}_3$, and so on (we examined
up to tenth order correlations, or $\mathbf{X}_{10}$).

One challenge to fairly evaluating high-order correlations is that if
the kernel used in Equation~\ref{eqn:timecorr} is wider than a single
timepoint, each repeated application of the equation will result in
further temporal blur.  Because our primary assessment metric is
temporal decoding accuracy, this unfairly biases against detecting
meaningful signal in higher-order correlations (relative to
lower-order correlations).  We attempted to mitigate temporal blur in
estimating each $\mathbf{X}_n$ by using a Dirac $\delta$ function
kernel (which places all of its mass over a single timepoint;
Fig.~\ref{fig:kernels}b) to compute each lower-order correlation
($\mathbf{X}_1, \mathbf{X}_2, ..., \mathbf{X}_{n-1}$).  We then used a
(potentially wider, as described below) kernel to compute $\mathbf{X}_{n}$ from
$\mathbf{X}_{n-1}$.  In this way, temporal blurring was applied only in the
last step of computing $\mathbf{X}_n$.  We note that, because each
$\mathbf{X}_n$ is a low-dimensional representation of the
corresponding $\mathbf{Y}_n$, the higher-order correlations we
estimated reflect true correlations in the data with lower-fidelity
than estimates of lower-order correlations.

After computing each
$\mathbf{X}_1, \mathbf{X}_2, ..., \mathbf{X}_{n-1}$ for each
participant, we divided participants into two equally sized groups
($\pm 1$ for odd numbers of participants):
$\mathcal{G}_{\mathrm{train}}$ and $\mathcal{G}_{\mathrm{test}}$.  We
then further subdivided $\mathcal{G}_{\mathrm{train}}$ into
$\mathcal{G}_{\mathrm{train}_1}$ and $\mathcal{G}_{\mathrm{train}_2}$.
We then computed $\mathbf{\Lambda}$ temporal correlation matrices for
each type of neural feature, using $\mathcal{G}_{\mathrm{train}_1}$
and $\mathcal{G}_{\mathrm{train}_2}$.  This resulted in $n+1$
$\mathbf{\Lambda}$ matrices (one for the original timeseries of neural
activations, and one for each of $n$ orders of dynamic correlations).
Our objective was to find a set of weights of each of these
$\mathbf{\Lambda}$ matrices such that the weighted average of the
$n+1$ matrices yielded the highest decoding accuracy.  We used
quasi-Newton gradient ascent~\citep{NoceWrig06}, using decoding
accuracy as the objective function to be maximized, to find an optimal
set of training data-derived weights, $\phi_{0, 1, ..., n}$, where
$\sum_{i=0}^n \phi_i = 1$ and where
$\phi_i \geq 0 \forall i \in \left[0, 1, ..., n\right]$.

After estimating an optimal set of weights, we computed a new set of
$n + 1$ $\mathbf{\Lambda}$ matrices correlating the DISFC patterns
from $\mathcal{G}_{\mathrm{train}}$ and $\mathcal{G}_{\mathrm{test}}$
at each timepoint.  We use the resulting decoding accuracy of
$\mathcal{G}_{\mathrm{test}}$ timepoints to estimate how informative
the set up neural features containing up to $n^\mathrm{th}$ order
correlations were.

We used a permutation-based procedure to form a stable estimate of
decoding accuracy for each set of neural features.  In particular, we
computed the decoding accuracy for each of 10 random group assignments of
$\mathcal{G}_{\mathrm{train}}$ and $\mathcal{G}_{\mathrm{test}}$.  We
report the mean accuracy (and 95\% confidence intervals) for each set
of neural features.


\subsubsection*{Identifying robust decoding results}
The temporal decoding procedure we use to estimate which neural
features support ongoing cognitive processing is goverened by many
parameters.  For example, Equation~\ref{eqn:timecorr} requires
defining a kernel function, which can take on different shapes and
widths.  For a fixed set of neural features, each of these parameters
can yield different decoding accuracies.  Further, the best decoding
accuracy for a given timepoint may be reliably achieved by one set of
parameters, whereas the best decoding accuracy for another timepoint
might be reliably achieved by a different set of parameters, and the
best decoding accuracy across \textit{all} timepoints might be
reliably achieved by still another different set of parameters.
Rather than attempting to maximize decoding accuracy, we sought to
discover the trends in the data that were robust to specific
classifier parameters choices.  Specifically, we sought to
characterize how decoding accuracy varied (under different
experimental conditions) as a function of which neural features were
considered.

To identify decoding results that were robust to specific classifier
parameter choices, we repeated our decoding analyses that substituted
in a variety of kernel shapes and widths for
Equation~\ref{eqn:timecorr}.  We examined Gaussian
(Fig.~\ref{fig:kernels}c), Laplace (Fig.~\ref{fig:kernels}d), and
Mexican Hat (Fig.~\ref{fig:kernels}e) kernels, each with widths of 5,
10, 20, and 50 samples.  We then report the average decoding
accuracies across all of these parameter choices.  This enabled us to
(roughly) factor out performance characteristics that were parameter
dependent (within the space of parameters we examined).

\textbf{JRM STOPPED HERE}

\subsubsection*{Reverse inference}
top 10 strongest correlations for each level; identify endpoint nodes
(RBFs); make map (threshold at $\theta$-- \textbf{JRM NOTE: what
  threshold did you use, Lucy?}); decode using neurosynth (cite).



\section*{Results}
\subsubsection*{Synthetic data}


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/synthetic_data.pdf}
  \caption{\textbf{Dynamic correlation recovery with synthetic data. } Using synthetic data containing different
    underlying correlational structure,
   we test how well we can recover dynamic correlation matrices using
   different kernels when compared to groud truth.  We compare the results using a delta kernel
   with averaged results from several kernels (Gaussian, Laplace, and
   mexican hat) and several widths (5, 10, 20, and 50).  We plot recovery using of datasets containing the
    following underlying structure:
    \textbf{a. Constant. b. Random. c. Ramping.  d. Block.}}
  \label{fig:synthetic_data}
\end{figure}

To assess the performance of dynamic correlation recovery using
timecorr, we varied width the kernel and the specific structure of the
data. We applied timecorr, using delta and gaussian kernels
Fig.~\ref{fig:kernels}) to each of the following  
synthetic datasets: constant, random, ramping, and block.  We then correlated each recovered
correlation matrix with the ground truth. 

For the constant synthetic dataset, a gaussian kernel (width=10)
outperformed the delta kernel (Fig.~\ref{fig:synthetic_data},  a.).  This is in contrast with the random
synthetic dataset, for which the delta kernel best captures the
rapidly changing structure (Fig.~\ref{fig:synthetic_data},  b.). For
the ramping synthetic dataset, the slow changing strucutre within the
data is best
captured by the gaussian kernel and the best recovery occurs in the
middle (Ramping, Fig.~\ref{fig:synthetic_data},
c.). In addition to comparing the timecorr recovered correlation
matrices to the ground truth, we
further compared the ramping recovered correlation matrices to only the first random covariance matrix $K_{1}$
(First, Fig.~\ref{fig:synthetic_data},  c.) and to only the last
random covariance matrix $K_{2}$ (Last, Fig.~\ref{fig:synthetic_data},
c.), both of which perform best at the beginning and end respectively.

Similary for the block sythetic dataset, we compared the timecorr
recovered correlation matrices to the ground truth as well as to each
block-specific covariance matrix (Block 1-5,
Fig.~\ref{fig:synthetic_data},  d.).  Although the structure is
changing by block, the gaussian kernel once again outperforms the
delta kernel.  Performance does however drop near even boundaries for
when using the gaussian kernel. 



\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/decode_level.pdf}
  \caption{\textbf{Decoding by order.
      a.\&d. Relative decoding accuracy by order.} Ribbons of each
    color display cross-validated decoding performance averaged over
    all parameters for each condition (intact, paragraph, word,
    and rest) using PCA (a.) or eigenvector centrality (d.) to
    approximate correlations. Decoders were trained using increasingly more
    higher-order information and this ribbons are displayed relative
    to chance at 0. The red dots indicates
    maximum decoding accuracy for each condition. \textbf{b.\&e. Z-transformed decoding accuracy
      by order.}  We Z-transformed the decoding accuracy by order to better
    visualize the order with the maximum decoding accuracy for each
    condition using PCA (b.) or eigenvector centrality (e.) to
    approximate correlations. \textbf{c.\&f. Optimized weights.} Bar heights indicate the
    optimized mixing paramete $\phi$ of each contributing order up to
    and including the order with the maximum
    decoding accuracy for each contributing order using PCA (c.) or eigenvector centrality (f.) to
    approximate correlations.
  For the order with maximum decoding accuracy by condition, we show
  barplots of the optimized weights $\phi$  for each contributing
  order.}
  \label{fig:decoding_level}
\end{figure}


We next evaluated if our model of high-order correlations in brain activity can capture cognitively relevant brain patterns. We performed a decoding analysis, using cross validation to estimate (using other participants’ data) which parts of the story each weighted-mixture of higher-order brain activity pattern corresponded to (see \textit{Materials and methods}). We note that our primary goal was not to achieve perfect decoding accuracy, but rather to use decoding accuracy as a benchmark for assessing whether different neural features specifically capture cognitively relevant brain patterns.

Separately for each experimental condition, we divided participants
into two groups. For the zeroth order, we computed the mean factor
activity for each group.  For all subsequent orders up to the tenth
order, we computed the mean approximated dynamic ISFC of factor
activity for each group (see \textit{Materials and methods}), and
combined in a weighted mixutre with all previous orders
(i.e. cross-validation for the second
order contained a weighted-mixture of zeroth, first, and second order
(Fig.~\ref{fig:decoding_level}, c.\&f.).  For
each order, we correlated the group 1 activity patterns with group 2
activity patterns.  We then subdivided the group 1 to obtain an
optimal weighting parameter for each order’s correlation matrix using
the same cross validation method. We used the optimal weighting
parameters to obtain a weighted-mixture (see \textit{Materials and methods}) of each order’s correlation
matrix. Using these correlations, we labeled the group 1 timepoints
using the group 2 timepoints with which they were most highly
correlated; we then computed the proportion of correctly labeled group
1 timepoints. (We also performed the symmetric analysis whereby we
labeled the group 2 timepoints using the group 1 timepoints as a
template.) We repeated this procedure 100 times (randomly re-assigning
participants to the two groups each time) to obtain a distribution of
decoding accuracies for each experimental condition. There were 272
timepoints for paragraph condition, 300 timepoints for intact and word
conditions, and 400 timepoints for rest condition,  so chance
performance on this decoding test is was $\frac{1}{272}$,
$\frac{1}{300}$, and $\frac{1}{400}$ respectively.
 
We repeated this process for each set of parameters, varying kernel
type and width, and averaged over the reduction technique used to
approximate the higher-order correlations (PCA Fig.~\ref{fig:decoding_level},  a.-c. and eigenvector
centrality Fig.~\ref{fig:decoding_level},  d.-f.).  Since there is no ground truth in these analyses, and we did not know
which parameters best capture the data, we instead report a robustness
search by averaging over the parameters and reporting which results
consistently showed up across all parameters.
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/brain_plots.pdf}
  \caption{\textbf{Average correlations by order for the intact listening condition.} Using eigenvector
    centrality to approximate higher-order correlations for the
    intact, paragraph scrambled, word scrambled, and rest condition.
    We plot the
    strongest 50\% absolute value
    mean correlation for \textbf{a.-d. first order, e.-h. second order,
      i.-l. third order, and m.-p. fourth order }, representing the degree of agreement by
    location pair over time.  To demonstrate how this method is
    computationally scalable, we also approximated
    \textbf{a.-d. fifteenth order} dynamic correlation, which would be
    possible to compute using conventional methods since it would require more bits to represent the solution than there are molecules in the universe.}
  \label{fig:brain_plots}
\end{figure}

The two methods used to approximate the higher-order correlations (PCA Fig.~\ref{fig:decoding_level},  a.-c. and eigenvector
centrality Fig.~\ref{fig:decoding_level},  d.-f.) capture different
facets of the activity patterns.  Using PCA, the higher-order
correlations are all linked to the original activity patterns, whereas
eigenvectory centrality breaks the immediate link with specific brain
areas and instead characterizes the position of the nodes in the
network that are similar over time.

We found for both PCA and eigenvector centrality, during the intact
condition in the experiment, classifiers that incorporated
higher-order correlations yielded consistently higher accuracy than
classifiers trained only on lower-order patterns
(Fig.~\ref{fig:decoding_level}, a.\&d.).  We plot the average
correlations for up to the fourth order for the intact condition
(Fig.~\ref{fig:brain_plots}) representing the degree of agreement by
location pair over time.  By contrast, we found that incorporating
higher-order (greater than first order) correlations did not further
improve decoding accuracy for the other listening conditions or rest
condition.  This suggests that the cognitive processing that supported
the most cognitively rich condition involved higher-order network
dynamics.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Discussion}

\begin{itemize}
  \item Methods advances: kernel-based dynamic correlations, extension to
    dynamic ISFC, efficient method for estimating high-order dynamic
    correlations, identifying robust results by averaging
  \item Discoveries:
    \begin{itemize}
      \item Dimensionality reduction and graph theoretic approaches
        give different insights into the data and identify different
        patterns as being relevant to cognition (different peak
        orders).
        \item An insight common to both approaches is that high-order
          (greater than first order) dynamic correlations are
          informative about ongoing high-level cognitive processing.
          As the level of cognitive processing decreases, cognition is
          reflected by lower-order correlations.
        \item Correlations at different orders are also associated
          with different networks of brain regions.  However, which
          networks reflect which types of interactions depends on the
          current task.  In general, lower order correlations during
          auditory listening reflect processing of low-level
          (auditory) features; mid-order correlations reflect speech
          and linguistic processing; higher-order correlations reflect
          across-sensory integration (e.g. ties to visual areas) and
          cognitive control areas.  This hierarchy dissolves during
          lower-order cognitive processing.
      \end{itemize}
  \end{itemize}


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/direction_of_field.pdf}
  \caption{\textbf{Direction of the field (adapted from~\citep{Turk13}).} The evolution of fMRI analyses started with
    univariate activation, which refers to
    the average amplitude of BOLD activity evoked by events of an
    experimental condition. Next, multivariate classifiers are trained
    on patterns of activation across voxels to decode distributed
    representations for specific events. The next, resting
    connectivity, is the temporal correlation of one or more seed
    regions with the remainder of the brain during rest. Additionally,
    task-based connectivity examines how these correlations differ by
    cognitive state. Following this increasing trajectory of
    increasing complexity, full connectivity considers all pairwise
    correlations in the brain, most commonly at rest.  Next, dynamic
    full connectivity considers how full connectivity changes over
    time. Continuing this line of reasoning, we expect higher-order network dynamics might provide even richer insights into the neural basis of cognition.}
  \label{fig:direction_of_field}
\end{figure}



Based on prior work ~\citep{DemeEtal19} and following the direction of the field ~\citep{Turk13} we think our thoughts might be encoded in
dynamic network patterns, and possibly higher order network
patterns (Fig.~\ref{fig:direction_of_field}). We sought to test this hypothesis by developing an approach
to inferring high-order network dynamics from timeseries data. 

One challenge in studying dynamic interactions is the
computational resources required to calculate higher-order correlations. 
We developed a computationally tractable model of network dynamics (Fig.~\ref{fig:methods}) that takes in a feature
timeseries and outputs approximated first-order dynamics (i.e.,
dynamic functional correlations), second-order dynamics
(reflecting homologous networks that dynamically form and disperse),
and higher-order network dynamics (up to tenth-order dynamic
correlations).

We first validated our model using synthetic data, and explored how
recovery varied with different underlying data structures and kernels.   We then 
applied the approach to an fMRI dataset
~\citep{SimoEtal16} in which participants listened to an audio
recording of a story, as well as scrambled versions of the same story
(where the scrambling was applied at different temporal scales).  We
trained classifiers to take the output of the model and decode the
timepoint in the story (or scrambled story) that the participants were
listening to. We found that, during the intact listening condition in the
experiment, classifiers that incorporated higher-order correlations
yielded consistently higher accuracy than classifiers trained only on
lower-order patterns (Fig.~\ref{fig:decoding_level},  a.\&d.).  By contrast, these
higher-order correlations were not necessary to support decoding the other
listening conditions and (minimally
above chance) during a control rest condition.  This suggests
that the cognitive processing that supported the most cogntively rich listening conditions
involved second-order (or higher) network dynamics.

Although we found decoding accuracy was best when incorporating
higher-order network dynamics for all but rest
  condition, it is unclear if this is a product of the brain or the
  data collection technique.  It could be that the brain is
  second-order or it could be that fMRI can
  only reliably give second-order interactions. Exploring this method
  with other data collection technique will be important to
  disentangle this question.



  \subsection*{Concluding remarks}

How can we better understand how brain patterns change over
time? How can we quantify the potential network dynamics that might be
driving these changes? One way to judge the techniques of the future is
to look at the trajectory of the fMRI field so far has taken so far
(Fig.~\ref{fig:methods}).  The field started with 
univariate activation, measuring the average activity for each voxel.
Analyses of multivariate activation followed, looking at spatial patterns of
activity over voxels. Next, correlations of activity were explored, first
with measures like resting connectivity that take temporal correlation
between a seed voxel and all other voxels then with full connectivty
that measure all pairwise correlations.  Additionally, this path of increasing
complexity also moved from static to dynamic measurements.  One
logical next step in this trajectory would be dynamic higher-order
correlations. We have created a method 
to support these calculations by scalably approximating dynamic higher-order
correlations.  

\section*{Acknowledgements}
We acknowledge discussions with Luke Chang, Hany Farid, Paxton
Fitzpatrick, Andrew Heusser, Eshin Jolly, Qiang Liu, Matthijs van der
Meer, Judith Mildner, Gina Notaro, Stephen Satterthwaite, Emily
Whitaker, Weizhen Xie, and Kirsten Ziman. Our work was supported in
part by NSF EPSCoR Award Number 1632738 to J.R.M. and by a sub-award
of DARPA RAM Cooperative Agreement N66001-14-2-4-032 to J.R.M.  The
content is solely the responsibility of the authors and does not
necessarily represent the official views of our supporting
organizations.

\section*{Author contributions}
Concept: J.R.M.  Implementation: T.H.C., L.L.W.O., and
J.R.M.  Analyses: L.L.W.O and J.R.M.

\bibliographystyle{apacite}
\bibliography{CDL-bibliography/memlab}

\end{document}


