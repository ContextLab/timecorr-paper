{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# from scipy.io import loadmat\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import timecorr as tc\n",
    "# import math\n",
    "# import scipy.spatial.distance as sd\n",
    "# from timecorr.helpers import isfc, autofc, mean_combine, corrmean_combine, vec2mat, reduce\n",
    "# corrsdir = '/Users/lucyowen/Desktop/corrs_for_j'\n",
    "# factors = 100\n",
    "\n",
    "# if factors == 100:\n",
    "#     pieman_name = '../../data/pieman_ica100.mat'\n",
    "# else:\n",
    "#     pieman_name = '../../data/pieman_data.mat'\n",
    "    \n",
    "\n",
    "# pieman_data = loadmat(pieman_name)  \n",
    "\n",
    "# pieman_conds = ['word']\n",
    "# data = []\n",
    "# conds = []\n",
    "# for c in pieman_conds:\n",
    "#     if c == 'paragraph':\n",
    "#         if factors == 700:\n",
    "#             next_data = list(map(lambda i: pieman_data[c][:, i][0], np.where(np.arange(pieman_data[c].shape[1]) != 3)[0]))\n",
    "#         else:\n",
    "#             next_data = list(map(lambda i: pieman_data[c][:, i][0], np.where(np.arange(pieman_data[c].shape[1]) != 0)[0]))\n",
    "#     else:\n",
    "#         next_data = list(map(lambda i: pieman_data[c][:, i][0], np.arange(pieman_data[c].shape[1])))\n",
    "#     data.extend(next_data)\n",
    "#     conds.extend([c]*len(next_data))\n",
    "\n",
    "# all_data = np.array(data)\n",
    "# conds = np.array(conds)\n",
    "# cfun = isfc\n",
    "# rfun = 'eigenvector_centrality'\n",
    "# width = 10\n",
    "# wp = 'gaussian'\n",
    "# cond = 'intact'\n",
    "# level = 1\n",
    "\n",
    "# gaussian = {'name': 'Gaussian', 'weights': tc.gaussian_weights, 'params': {'var': width}}\n",
    "\n",
    "# weights_paramter = eval(wp)\n",
    "\n",
    "\n",
    "# weights_fun=weights_paramter['weights']\n",
    "# weights_params=weights_paramter['params']\n",
    "# combine = corrmean_combine  \n",
    "# group_chunks = 2\n",
    "\n",
    "\n",
    "# ## just for word group 2\n",
    "# #### RUN FOR 700 FACTORS ON CLUSTER\n",
    "# for c in pieman_conds:\n",
    "#     g_inds = np.array_split(list(range(np.shape(all_data[conds == c])[0])), group_chunks)\n",
    "#     for i in range(2):\n",
    "#         data_1 = np.asarray(tc.timecorr([x for x in all_data[conds == c][g_inds[i]]], cfun=isfc, rfun=rfun,\n",
    "#                                                      weights_function=weights_fun, weights_params=weights_params))\n",
    "#         lev = np.asarray(tc.timecorr([x for x in data_1], cfun=isfc, rfun=None,\n",
    "#                                                      weights_function=weights_fun, weights_params=weights_params))\n",
    "#         corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(lev, axis=2)), axis=2))\n",
    "#         np.save(os.path.join(corrsdir, 'lev_2_'+c+'_'+str(i+1)+'.npy'), corr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_up(data, level=0, cfun=None, weights_fun=None, weights_params=None, combine=None, rfun=None):\n",
    "\n",
    "    from .timecorr import timecorr\n",
    "\n",
    "    if rfun is None:\n",
    "        rfun = [None] * np.shape(level)[0]\n",
    "\n",
    "    if level == 0:\n",
    "\n",
    "        smooth = np.asarray(timecorr([x for x in data], cfun=None,\n",
    "                                             rfun=rfun[level], combine=combine[level], weights_function=weights_fun,\n",
    "                                             weights_params=weights_params))\n",
    "        raw = mean_combine([x for x in data])\n",
    "        \n",
    "    else:\n",
    "        smooth = np.asarray(timecorr(data, cfun=cfun[level], rfun=rfun[level], combine=combine[level],\n",
    "                                                 weights_function=weights_fun, weights_params=weights_params))\n",
    "        raw = np.asarray(timecorr(data, cfun=cfun[level], rfun=rfun[level], combine=null_combine,\n",
    "                                              weights_function=eye_weights, weights_params=eye_params))\n",
    "\n",
    "\n",
    "    return smooth, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_wrap(data, dims=10, level=0, rfun=None):\n",
    "\n",
    "    if not level == 0:\n",
    "\n",
    "        all_smooth = list(data[0][np.newaxis, :, :]) + list(data[1][np.newaxis, :, :])\n",
    "        all_raw = list(data[2][np.newaxis, :, :]) + list(data[3][np.newaxis, :, :])\n",
    "\n",
    "        all_smooth_reduced = reduce(all_smooth, rfun=rfun[level])\n",
    "        all_raw_reduced = reduce(all_raw, rfun=rfun[level])\n",
    "\n",
    "\n",
    "        return all_smooth_reduced[0], all_smooth_reduced[1], all_raw_reduced[0], all_raw_reduced[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        return data[0], data[1], data[2], data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 3)\n",
      "0_data loaded\n",
      "0_reduced data loaded\n",
      "lev_data: [0.14302537 0.00982672 0.08145851 0.05860905 0.07720654 0.13107708\n",
      " 0.08700501 0.12014306 0.02272707 0.12851244 0.09639177 0.13539257\n",
      " 0.01057133 0.13790108 0.10585685 0.1308951  0.00367713 0.12067527\n",
      " 0.09668716 0.12913936 0.128724   0.07483774 0.0746319  0.02725164\n",
      " 0.08344455 0.12127144 0.08733651 0.14921866 0.02892407 0.12531941\n",
      " 0.00920434 0.10757915 0.00811552 0.03756734 0.11342522 0.07662683\n",
      " 0.13405174 0.05758245 0.02685659 0.12033433 0.06650679 0.05281363\n",
      " 0.1424802  0.10382324 0.0140855  0.04675881 0.09704961 0.1055391\n",
      " 0.05488077 0.13526464 0.02879268 0.00695268 0.08356988 0.02071912\n",
      " 0.03402362 0.13897895 0.1128056  0.11573054 0.06335576 0.07580266\n",
      " 0.13779102 0.1006125  0.13705344 0.12579201 0.04377354 0.11815308\n",
      " 0.10788544 0.13513238 0.12071263 0.11163909 0.12464418 0.13629674\n",
      " 0.09941703 0.13384385 0.07131796 0.12588853 0.12783232 0.0322023\n",
      " 0.06038959 0.11707526 0.08449785 0.11743815 0.12465436 0.12224591\n",
      " 0.12178867 0.09948445 0.09592106 0.1103323  0.10675676 0.07654605\n",
      " 0.08514327 0.11017971 0.10761328 0.11099025 0.11285975 0.10948922\n",
      " 0.11189975 0.1120059  0.06901239 0.11248397]\n",
      "data: [0.62200417 0.69342885 0.67412044 ... 0.52664712 0.53187254 0.57161806]\n",
      "1_smooth data calculated\n",
      "1_corrs calculated\n",
      "1_raw data calculated\n",
      "1_reduced data calculated\n",
      "data_r: [0.12777451 0.1436159  0.03540621 0.1045368  0.04934461 0.1115092\n",
      " 0.00712419 0.0835019  0.13715273 0.1065477  0.01513962 0.09677639\n",
      " 0.13165288 0.14203186 0.02727454 0.10657171 0.18040787 0.0639392\n",
      " 0.03992806 0.12860056 0.08598305 0.07612723 0.08606539 0.08853803\n",
      " 0.00647792 0.03968858 0.06676713 0.15981372 0.15829898 0.08838291\n",
      " 0.14795039 0.08180039 0.12761557 0.16327265 0.15019806 0.01082229\n",
      " 0.12794568 0.03236898 0.03312731 0.04779427 0.05590461 0.11481375\n",
      " 0.15280815 0.09503416 0.07490204 0.07746254 0.11509251 0.06059048\n",
      " 0.13201694 0.04121913 0.05514568 0.11546901 0.04439949 0.01198464\n",
      " 0.12427324 0.12027755 0.02418786 0.10972503 0.02930536 0.12291695\n",
      " 0.11948066 0.01102353 0.13395125 0.13975706 0.00448198 0.04359289\n",
      " 0.01994558 0.15000194 0.08701182 0.07194556 0.06505584 0.14996284\n",
      " 0.0540923  0.16832065 0.07227522 0.12853358 0.17261915 0.13517143\n",
      " 0.03915584 0.07174149 0.09793906 0.10588457 0.10792801 0.09706269\n",
      " 0.10372851 0.07311242 0.08265917 0.07949094 0.07778568 0.03916498\n",
      " 0.03918448 0.07279833 0.04849644 0.11997931 0.11350704 0.11372776\n",
      " 0.13516388 0.10263329 0.09982047 0.11310489]\n",
      "lev_data: [0.12777451 0.1436159  0.03540621 0.1045368  0.04934461 0.1115092\n",
      " 0.00712419 0.0835019  0.13715273 0.1065477  0.01513962 0.09677639\n",
      " 0.13165288 0.14203186 0.02727454 0.10657171 0.18040787 0.0639392\n",
      " 0.03992806 0.12860056 0.08598305 0.07612723 0.08606539 0.08853803\n",
      " 0.00647792 0.03968858 0.06676713 0.15981372 0.15829898 0.08838291\n",
      " 0.14795039 0.08180039 0.12761557 0.16327265 0.15019806 0.01082229\n",
      " 0.12794568 0.03236898 0.03312731 0.04779427 0.05590461 0.11481375\n",
      " 0.15280815 0.09503416 0.07490204 0.07746254 0.11509251 0.06059048\n",
      " 0.13201694 0.04121913 0.05514568 0.11546901 0.04439949 0.01198464\n",
      " 0.12427324 0.12027755 0.02418786 0.10972503 0.02930536 0.12291695\n",
      " 0.11948066 0.01102353 0.13395125 0.13975706 0.00448198 0.04359289\n",
      " 0.01994558 0.15000194 0.08701182 0.07194556 0.06505584 0.14996284\n",
      " 0.0540923  0.16832065 0.07227522 0.12853358 0.17261915 0.13517143\n",
      " 0.03915584 0.07174149 0.09793906 0.10588457 0.10792801 0.09706269\n",
      " 0.10372851 0.07311242 0.08265917 0.07949094 0.07778568 0.03916498\n",
      " 0.03918448 0.07279833 0.04849644 0.11997931 0.11350704 0.11372776\n",
      " 0.13516388 0.10263329 0.09982047 0.11310489]\n",
      "data: [0.67397836 0.73286064 0.73245235 ... 0.59628653 0.64426353 0.5868118 ]\n",
      "2_smooth data calculated\n",
      "2_corrs calculated\n",
      "2_raw data calculated\n",
      "2_reduced data calculated\n",
      "data_r: [0.11673606 0.12467842 0.10592967 0.09152751 0.07525425 0.09444332\n",
      " 0.11346492 0.02522004 0.13673002 0.10488391 0.10014631 0.09172004\n",
      " 0.094433   0.1448585  0.1049608  0.09677053 0.15573613 0.00863871\n",
      " 0.08445421 0.13595562 0.05528651 0.0328017  0.07307202 0.03048183\n",
      " 0.07925093 0.03742451 0.03248871 0.15521049 0.14432928 0.01620245\n",
      " 0.11201054 0.00628171 0.07933924 0.15870555 0.159953   0.12994089\n",
      " 0.05949171 0.0093096  0.09770757 0.12192155 0.02050832 0.1394284\n",
      " 0.15184725 0.08694812 0.07882932 0.07953889 0.12230636 0.10077552\n",
      " 0.13930028 0.13611066 0.12027196 0.03148404 0.10335925 0.10675253\n",
      " 0.01631729 0.00716237 0.10651694 0.02776696 0.1436251  0.1309802\n",
      " 0.01499296 0.15233458 0.09767196 0.12663231 0.1408224  0.14362938\n",
      " 0.14701552 0.1194564  0.07778671 0.10784148 0.12805954 0.11789537\n",
      " 0.10163141 0.14123189 0.01932326 0.07916642 0.14808455 0.09046641\n",
      " 0.11324478 0.10453031 0.02862762 0.00624307 0.01180892 0.03920942\n",
      " 0.00973934 0.09035293 0.0678416  0.09708598 0.09696435 0.14141274\n",
      " 0.13593582 0.09812196 0.13601046 0.06071657 0.02817318 0.03333612\n",
      " 0.09951089 0.00557695 0.02141214 0.03370614]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import timecorr as tc\n",
    "import math\n",
    "import scipy.spatial.distance as sd\n",
    "from timecorr.helpers import isfc, autofc, mean_combine, corrmean_combine, vec2mat, reduce\n",
    "\n",
    "factors = 100\n",
    "if factors == 100:\n",
    "    pieman_name = '../../data/pieman_ica100.mat'\n",
    "else:\n",
    "    pieman_name = '../../data/pieman_data.mat'\n",
    "\n",
    "\n",
    "\n",
    "pieman_data = loadmat(pieman_name)  \n",
    "\n",
    "pieman_conds = ['rest']\n",
    "\n",
    "data = []\n",
    "conds = []\n",
    "\n",
    "\n",
    "for c in pieman_conds:\n",
    "    if c == 'paragraph':\n",
    "        if factors == 700:\n",
    "            next_data = list(map(lambda i: pieman_data[c][:, i][0], np.where(np.arange(pieman_data[c].shape[1]) != 3)[0]))\n",
    "        else:\n",
    "            next_data = list(map(lambda i: pieman_data[c][:, i][0], np.where(np.arange(pieman_data[c].shape[1]) != 0)[0]))\n",
    "    else:\n",
    "        next_data = list(map(lambda i: pieman_data[c][:, i][0], np.arange(pieman_data[c].shape[1])))\n",
    "    data.extend(next_data)\n",
    "    conds.extend([c]*len(next_data))\n",
    "\n",
    "\n",
    "all_data = np.array(data)\n",
    "conds = np.array(conds)\n",
    "cfun = isfc\n",
    "rfun = 'eigenvector_centrality'\n",
    "width = 50\n",
    "smooth = 'laplace'\n",
    "raw = 'delta'\n",
    "\n",
    "#results_dir = '/dartfs/rc/lab/D/DBIC/CDL/f002s72/timecorr_paper/pieman/results/'\n",
    "\n",
    "results_dir = '/Users/lucyowen/Desktop/timecorr_env/timecorr_paper/pieman/results/'\n",
    "\n",
    "corrsdir = os.path.join(results_dir, wp + '_' + str(50) + '_orderedup_corrs')\n",
    "\n",
    "if not os.path.exists(corrsdir):\n",
    "    os.makedirs(corrsdir)\n",
    "    \n",
    "laplace = {'name': 'Laplace', 'weights': tc.laplace_weights, 'params': {'scale': width}}\n",
    "delta = {'name': '$\\delta$', 'weights': tc.eye_weights, 'params': tc.eye_params}\n",
    "gaussian = {'name': 'Gaussian', 'weights': tc.gaussian_weights, 'params': {'var': width}}\n",
    "mexican_hat = {'name': 'Mexican hat', 'weights': tc.mexican_hat_weights, 'params': {'sigma': width}}\n",
    "\n",
    "smooth_parameter = eval(smooth)\n",
    "raw_parameter = eval(raw)\n",
    "\n",
    "smooth_fun=smooth_parameter['weights']\n",
    "smooth_params=smooth_parameter['params']\n",
    "raw_fun=raw_parameter['weights']\n",
    "raw_params=raw_parameter['params']\n",
    "\n",
    "combine = corrmean_combine  \n",
    "\n",
    "levels = range(3)\n",
    "\n",
    "print(levels)\n",
    "\n",
    "for c in pieman_conds:\n",
    "\n",
    "    for lev in levels:\n",
    "        \n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_'+ str(lev) +'_'+ c +'.npy')):\n",
    "            data = np.load(os.path.join(corrsdir, 'd_'+ str(lev) +'_'+ c +'.npy'))\n",
    "            print(str(lev) + '_data loaded')\n",
    "        else:\n",
    "            \n",
    "            if lev == 0:\n",
    "                lev_data = all_data[conds == c]\n",
    "            else:\n",
    "                lev_data = data_r\n",
    "                \n",
    "            print('lev_data: ' + str(lev_data[0][0]))\n",
    "            \n",
    "            data = np.asarray(tc.timecorr([x for x in lev_data], cfun=isfc, rfun=None,\n",
    "                                          weights_function=smooth_fun, weights_params=smooth_params))\n",
    "            \n",
    "            print('data: ' + str(data[0][0]))\n",
    "            print(str(lev) + '_smooth data calculated')\n",
    "            \n",
    "            corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(data, axis=2)), axis=2))\n",
    "            \n",
    "            print(str(lev) + '_corrs calculated')\n",
    "            \n",
    "            np.save(os.path.join(corrsdir, 'lev_'+ str(lev) +'_'+ c +'.npy'), corr)\n",
    "            \n",
    "            data = np.asarray(tc.timecorr([x for x in lev_data], cfun=isfc, rfun=None,\n",
    "                                          weights_function=raw_fun, weights_params=raw_params))\n",
    "            \n",
    "            print(str(lev) + '_raw data calculated')\n",
    "            \n",
    "            np.save(os.path.join(corrsdir, 'd_'+ str(lev) +'_'+ c +'.npy'), data)\n",
    "            \n",
    "\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_'+ str(lev) +'_r_'+ c +'.npy')):\n",
    "            data_r = np.load(os.path.join(corrsdir, 'd_'+ str(lev) +'_r_'+ c +'.npy'))\n",
    "            print(str(lev) + '_reduced data loaded')\n",
    "        else:\n",
    "            data_r = np.asarray(reduce([x for x in data], rfun=rfun))\n",
    "            print(str(lev) + '_reduced data calculated')\n",
    "            np.save(os.path.join(corrsdir, 'd_'+ str(lev) +'_r_'+ c +'.npy'), data_r)\n",
    "\n",
    "            print('data_r: ' + str(data_r[0][0]))\n",
    "        del data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = np.load(os.path.join(corrsdir, 'lev_0_rest.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00472924,  0.00292534, -0.00800761, ..., -0.03615471,\n",
       "         0.00085978,  0.02894596],\n",
       "       [ 0.00473935,  0.00292719, -0.00800845, ..., -0.0361507 ,\n",
       "         0.00084322,  0.02895373],\n",
       "       [ 0.00474985,  0.00292952, -0.00800761, ..., -0.0361429 ,\n",
       "         0.00082702,  0.02896362],\n",
       "       ...,\n",
       "       [ 0.00470163,  0.00324716, -0.00783575, ..., -0.03603504,\n",
       "         0.00084905,  0.02891161],\n",
       "       [ 0.00469569,  0.00323683, -0.00785285, ..., -0.03603852,\n",
       "         0.00084916,  0.02891328],\n",
       "       [ 0.00469079,  0.00322541, -0.0078637 , ..., -0.03604023,\n",
       "         0.00085111,  0.02891425]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.load(os.path.join(corrsdir, 'lev_1_rest.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67791692, 0.69482554, 0.68652665, ..., 0.51751886, 0.52131083,\n",
       "        0.56299987],\n",
       "       [0.67062577, 0.68782968, 0.67891135, ..., 0.50852289, 0.51226615,\n",
       "        0.55458279],\n",
       "       [0.66328066, 0.68075285, 0.67115112, ..., 0.49944956, 0.50336628,\n",
       "        0.54605204],\n",
       "       ...,\n",
       "       [0.64311738, 0.66809795, 0.67043827, ..., 0.51054083, 0.47364042,\n",
       "        0.55140913],\n",
       "       [0.65128266, 0.67576094, 0.67812503, ..., 0.51882848, 0.48392795,\n",
       "        0.55976356],\n",
       "       [0.65940173, 0.68335293, 0.68574443, ..., 0.527367  , 0.49420536,\n",
       "        0.5680266 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = np.load(os.path.join(corrsdir, 'lev_2_rest.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72958944, 0.73361583, 0.73575896, ..., 0.59141414, 0.64253216,\n",
       "        0.58702491],\n",
       "       [0.72226216, 0.72648828, 0.72870467, ..., 0.5814949 , 0.63364393,\n",
       "        0.57788729],\n",
       "       [0.71487135, 0.71926916, 0.72163213, ..., 0.57165385, 0.62468238,\n",
       "        0.56883825],\n",
       "       ...,\n",
       "       [0.72768249, 0.72833249, 0.7266187 , ..., 0.59283252, 0.6746103 ,\n",
       "        0.56274283],\n",
       "       [0.73447836, 0.73511093, 0.73352724, ..., 0.60183649, 0.68140581,\n",
       "        0.57219168],\n",
       "       [0.74118228, 0.74180691, 0.74033674, ..., 0.61082572, 0.68831013,\n",
       "        0.58166346]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster code to calculate correlations for each order - up to 4th order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e38de72a2a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#             np.save(os.path.join(corrsdir, 'd_2_lev_3_'+c+'_'+str(i+1)+'.npy'), data_2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         lev = np.asarray(tc.timecorr([x for x in data_2], cfun=isfc, rfun=None,\n\u001b[0;32m---> 81\u001b[0;31m                                                      weights_function=weights_fun, weights_params=weights_params))\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrsdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lev_3_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/timecorr/timecorr/timecorr.py\u001b[0m in \u001b[0;36mtimecorr\u001b[0;34m(data, weights_function, weights_params, include_timepoints, exclude_timepoints, combine, cfun, rfun)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mreturn_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/timecorr/timecorr/helpers.py\u001b[0m in \u001b[0;36mformat_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_nans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/hypertools/tools/format_data.py\u001b[0m in \u001b[0;36mformat_data\u001b[0;34m(x, vectorizer, semantic, corpus, ppca, text_align)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# check data type for each element in list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mdtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# handle text data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/hypertools/_shared/helpers.py\u001b[0m in \u001b[0;36mget_type\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    207\u001b[0m                             ', List of numbers')\n\u001b[1;32m    208\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'arr_str'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import timecorr as tc\n",
    "import math\n",
    "import scipy.spatial.distance as sd\n",
    "from timecorr.helpers import isfc, autofc, mean_combine, corrmean_combine, vec2mat, reduce\n",
    "\n",
    "factors = 100\n",
    "if factors == 100:\n",
    "    pieman_name = '../../data/pieman_ica100.mat'\n",
    "else:\n",
    "    pieman_name = '../../data/pieman_data.mat'\n",
    "\n",
    "\n",
    "\n",
    "pieman_data = loadmat(pieman_name)  \n",
    "\n",
    "pieman_conds = ['rest']\n",
    "\n",
    "data = []\n",
    "conds = []\n",
    "\n",
    "\n",
    "for c in pieman_conds:\n",
    "    if c == 'paragraph':\n",
    "        if factors == 700:\n",
    "            next_data = list(map(lambda i: pieman_data[c][:, i][0], np.where(np.arange(pieman_data[c].shape[1]) != 3)[0]))\n",
    "        else:\n",
    "            next_data = list(map(lambda i: pieman_data[c][:, i][0], np.where(np.arange(pieman_data[c].shape[1]) != 0)[0]))\n",
    "    else:\n",
    "        next_data = list(map(lambda i: pieman_data[c][:, i][0], np.arange(pieman_data[c].shape[1])))\n",
    "    data.extend(next_data)\n",
    "    conds.extend([c]*len(next_data))\n",
    "\n",
    "\n",
    "all_data = np.array(data)\n",
    "conds = np.array(conds)\n",
    "cfun = isfc\n",
    "rfun = 'eigenvector_centrality'\n",
    "width = 50\n",
    "wp = 'laplace'\n",
    "\n",
    "#results_dir = '/dartfs/rc/lab/D/DBIC/CDL/f002s72/timecorr_paper/pieman/results/'\n",
    "\n",
    "results_dir = '/Users/lucyowen/Desktop/timecorr_env/timecorr_paper/pieman/results/'\n",
    "\n",
    "corrsdir = os.path.join(results_dir, wp + '_' + str(50) + '_orderedup_corrs')\n",
    "\n",
    "if not os.path.exists(corrsdir):\n",
    "    os.makedirs(corrsdir)\n",
    "            \n",
    "laplace = {'name': 'Laplace', 'weights': tc.laplace_weights, 'params': {'scale': width}}\n",
    "delta = {'name': '$\\delta$', 'weights': tc.eye_weights, 'params': tc.eye_params}\n",
    "gaussian = {'name': 'Gaussian', 'weights': tc.gaussian_weights, 'params': {'var': width}}\n",
    "mexican_hat = {'name': 'Mexican hat', 'weights': tc.mexican_hat_weights, 'params': {'sigma': width}}\n",
    "\n",
    "weights_paramter = eval(wp)\n",
    "\n",
    "\n",
    "weights_fun=weights_paramter['weights']\n",
    "weights_params=weights_paramter['params']\n",
    "combine = corrmean_combine  \n",
    "group_chunks = 2\n",
    "\n",
    "\n",
    "for c in pieman_conds:\n",
    "    g_inds = np.array_split(list(range(np.shape(all_data[conds == c])[0])), group_chunks)\n",
    "    for i in range(2):\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_1_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data = np.load(os.path.join(corrsdir, 'd_1_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data = np.asarray(tc.timecorr([x for x in all_data[conds == c][g_inds[i]]], cfun=isfc, rfun=None,\n",
    "                                                         weights_function=weights_fun, weights_params=weights_params))\n",
    "            corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(data_1, axis=2)), axis=2))\n",
    "            \n",
    "            data = np.asarray(tc.timecorr([x for x in all_data[conds == c][g_inds[i]]], cfun=isfc, rfun=,\n",
    "                                                         weights_function=weights_fun, weights_params=weights_params))\n",
    "            \n",
    "            np.save(os.path.join(corrsdir, 'd_1_'+c+'_'+str(i+1)+'.npy'), data_1)\n",
    "            np.save(os.path.join(corrsdir, 'lev_1_'+c+'_'+str(i+1)+'.npy'), corr)\n",
    "            \n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_1_r_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_r = np.load(os.path.join(corrsdir, 'd_1_r_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_r = np.asarray(reduce([x for x in data_1], rfun=rfun))\n",
    "            np.save(os.path.join(corrsdir, 'd_1_r_'+ c +'_'+str(i+1)+'.npy'), data_1_r)\n",
    "            \n",
    "        del data\n",
    "        \n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_2_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_2 = np.load(os.path.join(corrsdir, 'd_2_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_2 = np.asarray(tc.timecorr([x for x in data_1_r], cfun=isfc, rfun=None,\n",
    "                                                         weights_function=weights_fun, weights_params=weights_params))\n",
    "            corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(data_2, axis=2)), axis=2))\n",
    "            np.save(os.path.join(corrsdir, 'd_2_'+ c +'_'+str(i+1)+'.npy'), data_2)\n",
    "            np.save(os.path.join(corrsdir, 'lev_2_'+c+'_'+str(i+1)+'.npy'), corr)\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_2_r_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_2_r = np.load(os.path.join(corrsdir, 'd_2_r_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_2_r = np.asarray(reduce([x for x in data_2], rfun=rfun))\n",
    "            np.save(os.path.join(corrsdir, 'd_2_r_'+ c +'_'+str(i+1)+'.npy'), data_2_r)\n",
    "        del data_2\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_3_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_3 = np.load(os.path.join(corrsdir, 'd_3_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_3 = np.asarray(tc.timecorr([x for x in data_2_r], cfun=isfc, rfun=None,\n",
    "                                                         weights_function=weights_fun, weights_params=weights_params))\n",
    "            corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(data_3, axis=2)), axis=2))\n",
    "            np.save(os.path.join(corrsdir, 'd_3_'+ c +'_'+str(i+1)+'.npy'), data_3)\n",
    "            np.save(os.path.join(corrsdir, 'lev_3_'+c+'_'+str(i+1)+'.npy'), corr)\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_3_r_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_3_r = np.load(os.path.join(corrsdir, 'd_3_r_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_3_r = np.asarray(reduce([x for x in data_3], rfun=rfun))\n",
    "            np.save(os.path.join(corrsdir, 'd_3_r_'+ c +'_'+str(i+1)+'.npy'), data_3_r)\n",
    "        del data_3\n",
    "        lev = np.asarray(tc.timecorr([x for x in data_3_r], cfun=isfc, rfun=None,\n",
    "                                                     weights_function=weights_fun, weights_params=weights_params))\n",
    "        corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(lev, axis=2)), axis=2))\n",
    "        np.save(os.path.join(corrsdir, 'lev_4_'+c+'_'+str(i+1)+'.npy'), corr)\n",
    "        del lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import timecorr as tc\n",
    "import math\n",
    "import scipy.spatial.distance as sd\n",
    "from timecorr.helpers import isfc, autofc, mean_combine, corrmean_combine, vec2mat, reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/dartfs/rc/lab/D/DBIC/CDL/f002s72/timecorr_paper/pieman/results'\n",
    "corrs_dir = os.path.join(data_dir, 'laplace_50_corrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For sliced correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [1,2, 3, 4]\n",
    "groups = [1, 2]\n",
    "conditions = ['intact', 'paragraph', 'word', 'rest']\n",
    "\n",
    "n = .1\n",
    "percentages = np.arange(n, 1 + n, n)\n",
    "for l in levels:\n",
    "    for c in conditions:\n",
    "        for g in groups:\n",
    "            con = os.path.join(corrs_dir, f'lev_{l}'+ f'_{c}'+ f'_{g}.npy')\n",
    "            try:\n",
    "                corrs = np.load(con)\n",
    "                mat_corrs = tc.helpers.vec2mat(corrs)\n",
    "                sliced_corrs = np.array([])\n",
    "                next_corrdir = os.path.join(data_dir, 'sliced_corrs', f'level_{l}', f'group {g}')\n",
    "                if not os.path.exists(next_corrdir):\n",
    "                    os.makedirs(next_corrdir)\n",
    "                for p in percentages:\n",
    "                    s = int(p*np.shape(mat_corrs)[-1])\n",
    "                    slice_corr = mat_corrs[:, :, s-1]\n",
    "                    if sliced_corrs.shape[0] == 0:\n",
    "                        sliced_corrs = slice_corr\n",
    "                    else:\n",
    "                        sliced_corrs = np.dstack((sliced_corrs, slice_corr))\n",
    "                np.save(os.path.join(next_corrdir, f'{c}.npy'), sliced_corrs)\n",
    "            except:\n",
    "                print('issue loading: ' + con)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For averaged correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [1,2, 3, 4]\n",
    "groups = [1, 2]\n",
    "conditions = ['intact', 'paragraph', 'word', 'rest']\n",
    "\n",
    "\n",
    "for l in levels:\n",
    "    for c in conditions:\n",
    "        for g in groups:\n",
    "            con = os.path.join(corrs_dir, f'lev_{l}'+ f'_{c}'+ f'_{g}.npy')\n",
    "            try:\n",
    "                corrs = np.load(con)\n",
    "                mat_corrs = tc.helpers.vec2mat(corrs)\n",
    "\n",
    "                next_corrdir = os.path.join(data_dir, 'mean_corrs', f'level_{l}', f'group {g}')\n",
    "                if not os.path.exists(next_corrdir):\n",
    "                    os.makedirs(next_corrdir)\n",
    "\n",
    "                mean_corrs = mat_corrs.mean(axis=2)\n",
    "\n",
    "                np.save(os.path.join(next_corrdir, f'{c}.npy'), mean_corrs)\n",
    "            \n",
    "            except:\n",
    "                print('issue loading: ' + con)\n",
    "                pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
