{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# from scipy.io import loadmat\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import timecorr as tc\n",
    "# import math\n",
    "# import scipy.spatial.distance as sd\n",
    "# from timecorr.helpers import isfc, autofc, mean_combine, corrmean_combine, vec2mat, reduce\n",
    "# corrsdir = '/Users/lucyowen/Desktop/corrs_for_j'\n",
    "# factors = 100\n",
    "\n",
    "# if factors == 100:\n",
    "#     pieman_name = '../../data/pieman_ica100.mat'\n",
    "# else:\n",
    "#     pieman_name = '../../data/pieman_data.mat'\n",
    "    \n",
    "\n",
    "# pieman_data = loadmat(pieman_name)  \n",
    "\n",
    "# pieman_conds = ['word']\n",
    "# data = []\n",
    "# conds = []\n",
    "# for c in pieman_conds:\n",
    "#     if c == 'paragraph':\n",
    "#         if factors == 700:\n",
    "#             next_data = list(map(lambda i: pieman_data[c][:, i][0], np.where(np.arange(pieman_data[c].shape[1]) != 3)[0]))\n",
    "#         else:\n",
    "#             next_data = list(map(lambda i: pieman_data[c][:, i][0], np.where(np.arange(pieman_data[c].shape[1]) != 0)[0]))\n",
    "#     else:\n",
    "#         next_data = list(map(lambda i: pieman_data[c][:, i][0], np.arange(pieman_data[c].shape[1])))\n",
    "#     data.extend(next_data)\n",
    "#     conds.extend([c]*len(next_data))\n",
    "\n",
    "# all_data = np.array(data)\n",
    "# conds = np.array(conds)\n",
    "# cfun = isfc\n",
    "# rfun = 'eigenvector_centrality'\n",
    "# width = 10\n",
    "# wp = 'gaussian'\n",
    "# cond = 'intact'\n",
    "# level = 1\n",
    "\n",
    "# gaussian = {'name': 'Gaussian', 'weights': tc.gaussian_weights, 'params': {'var': width}}\n",
    "\n",
    "# weights_paramter = eval(wp)\n",
    "\n",
    "\n",
    "# weights_fun=weights_paramter['weights']\n",
    "# weights_params=weights_paramter['params']\n",
    "# combine = corrmean_combine  \n",
    "# group_chunks = 2\n",
    "\n",
    "\n",
    "# ## just for word group 2\n",
    "# #### RUN FOR 700 FACTORS ON CLUSTER\n",
    "# for c in pieman_conds:\n",
    "#     g_inds = np.array_split(list(range(np.shape(all_data[conds == c])[0])), group_chunks)\n",
    "#     for i in range(2):\n",
    "#         data_1 = np.asarray(tc.timecorr([x for x in all_data[conds == c][g_inds[i]]], cfun=isfc, rfun=rfun,\n",
    "#                                                      weights_function=weights_fun, weights_params=weights_params))\n",
    "#         lev = np.asarray(tc.timecorr([x for x in data_1], cfun=isfc, rfun=None,\n",
    "#                                                      weights_function=weights_fun, weights_params=weights_params))\n",
    "#         corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(lev, axis=2)), axis=2))\n",
    "#         np.save(os.path.join(corrsdir, 'lev_2_'+c+'_'+str(i+1)+'.npy'), corr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster code to calculate correlations for each order - up to 4th order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e38de72a2a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#             np.save(os.path.join(corrsdir, 'd_2_lev_3_'+c+'_'+str(i+1)+'.npy'), data_2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         lev = np.asarray(tc.timecorr([x for x in data_2], cfun=isfc, rfun=None,\n\u001b[0;32m---> 81\u001b[0;31m                                                      weights_function=weights_fun, weights_params=weights_params))\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrsdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lev_3_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/timecorr/timecorr/timecorr.py\u001b[0m in \u001b[0;36mtimecorr\u001b[0;34m(data, weights_function, weights_params, include_timepoints, exclude_timepoints, combine, cfun, rfun)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mreturn_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/timecorr/timecorr/helpers.py\u001b[0m in \u001b[0;36mformat_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_nans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/hypertools/tools/format_data.py\u001b[0m in \u001b[0;36mformat_data\u001b[0;34m(x, vectorizer, semantic, corpus, ppca, text_align)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# check data type for each element in list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mdtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# handle text data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/hypertools/_shared/helpers.py\u001b[0m in \u001b[0;36mget_type\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    207\u001b[0m                             ', List of numbers')\n\u001b[1;32m    208\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'arr_str'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import timecorr as tc\n",
    "import math\n",
    "import scipy.spatial.distance as sd\n",
    "from timecorr.helpers import isfc, autofc, mean_combine, corrmean_combine, vec2mat, reduce\n",
    "\n",
    "factors = 700\n",
    "if factors == 100:\n",
    "    pieman_name = '../../data/pieman_ica100.mat'\n",
    "else:\n",
    "    pieman_name = '../../data/pieman_data.mat'\n",
    "\n",
    "\n",
    "\n",
    "pieman_data = loadmat(pieman_name)  \n",
    "\n",
    "pieman_conds = ['rest']\n",
    "\n",
    "data = []\n",
    "conds = []\n",
    "\n",
    "\n",
    "for c in pieman_conds:\n",
    "    if c == 'paragraph':\n",
    "        if factors == 700:\n",
    "            next_data = list(map(lambda i: pieman_data[c][:, i][0], np.where(np.arange(pieman_data[c].shape[1]) != 3)[0]))\n",
    "        else:\n",
    "            next_data = list(map(lambda i: pieman_data[c][:, i][0], np.where(np.arange(pieman_data[c].shape[1]) != 0)[0]))\n",
    "    else:\n",
    "        next_data = list(map(lambda i: pieman_data[c][:, i][0], np.arange(pieman_data[c].shape[1])))\n",
    "    data.extend(next_data)\n",
    "    conds.extend([c]*len(next_data))\n",
    "\n",
    "\n",
    "all_data = np.array(data)\n",
    "conds = np.array(conds)\n",
    "cfun = isfc\n",
    "rfun = 'eigenvector_centrality'\n",
    "width = 50\n",
    "wp = 'laplace'\n",
    "\n",
    "results_dir = '/dartfs/rc/lab/D/DBIC/CDL/f002s72/timecorr_paper/pieman/results/'\n",
    "\n",
    "corrsdir = os.path.join(results_dir, wp + '_' + str(50) + '_corrs')\n",
    "\n",
    "laplace = {'name': 'Laplace', 'weights': tc.laplace_weights, 'params': {'scale': width}}\n",
    "\n",
    "weights_paramter = eval(wp)\n",
    "\n",
    "\n",
    "weights_fun=weights_paramter['weights']\n",
    "weights_params=weights_paramter['params']\n",
    "combine = corrmean_combine  \n",
    "group_chunks = 2\n",
    "\n",
    "\n",
    "for c in pieman_conds:\n",
    "    g_inds = np.array_split(list(range(np.shape(all_data[conds == c])[0])), group_chunks)\n",
    "    for i in range(2):\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_1_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_1 = np.load(os.path.join(corrsdir, 'd_1_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_1 = np.asarray(tc.timecorr([x for x in all_data[conds == c][g_inds[i]]], cfun=isfc, rfun=None,\n",
    "                                                         weights_function=weights_fun, weights_params=weights_params))\n",
    "            corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(data_1, axis=2)), axis=2))\n",
    "            np.save(os.path.join(corrsdir, 'd_1_'+c+'_'+str(i+1)+'.npy'), data_1)\n",
    "            np.save(os.path.join(corrsdir, 'lev_1_'+c+'_'+str(i+1)+'.npy'), corr)\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_1_r_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_1_r = np.load(os.path.join(corrsdir, 'd_1_r_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_1_r = np.asarray(reduce([x for x in data_1], rfun=rfun))\n",
    "            np.save(os.path.join(corrsdir, 'd_1_r_'+ c +'_'+str(i+1)+'.npy'), data_1_r)\n",
    "        del data_1\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_2_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_2 = np.load(os.path.join(corrsdir, 'd_2_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_2 = np.asarray(tc.timecorr([x for x in data_1_r], cfun=isfc, rfun=None,\n",
    "                                                         weights_function=weights_fun, weights_params=weights_params))\n",
    "            corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(data_2, axis=2)), axis=2))\n",
    "            np.save(os.path.join(corrsdir, 'd_2_'+ c +'_'+str(i+1)+'.npy'), data_2)\n",
    "            np.save(os.path.join(corrsdir, 'lev_2_'+c+'_'+str(i+1)+'.npy'), corr)\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_2_r_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_2_r = np.load(os.path.join(corrsdir, 'd_2_r_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_2_r = np.asarray(reduce([x for x in data_2], rfun=rfun))\n",
    "            np.save(os.path.join(corrsdir, 'd_2_r_'+ c +'_'+str(i+1)+'.npy'), data_2_r)\n",
    "        del data_2\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_3_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_3 = np.load(os.path.join(corrsdir, 'd_3_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_3 = np.asarray(tc.timecorr([x for x in data_2_r], cfun=isfc, rfun=None,\n",
    "                                                         weights_function=weights_fun, weights_params=weights_params))\n",
    "            corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(data_3, axis=2)), axis=2))\n",
    "            np.save(os.path.join(corrsdir, 'd_3_'+ c +'_'+str(i+1)+'.npy'), data_3)\n",
    "            np.save(os.path.join(corrsdir, 'lev_3_'+c+'_'+str(i+1)+'.npy'), corr)\n",
    "        if os.path.exists(os.path.join(corrsdir, 'd_3_r_'+ c +'_'+str(i+1)+'.npy')):\n",
    "            data_3_r = np.load(os.path.join(corrsdir, 'd_3_r_'+ c +'_'+str(i+1)+'.npy'))\n",
    "        else:\n",
    "            data_3_r = np.asarray(reduce([x for x in data_3], rfun=rfun))\n",
    "            np.save(os.path.join(corrsdir, 'd_3_r_'+ c +'_'+str(i+1)+'.npy'), data_3_r)\n",
    "        del data_3\n",
    "        lev = np.asarray(tc.timecorr([x for x in data_3_r], cfun=isfc, rfun=None,\n",
    "                                                     weights_function=weights_fun, weights_params=weights_params))\n",
    "        corr = tc.helpers.z2r(np.mean(tc.helpers.r2z(np.stack(lev, axis=2)), axis=2))\n",
    "        np.save(os.path.join(corrsdir, 'lev_4_'+c+'_'+str(i+1)+'.npy'), corr)\n",
    "        del lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import timecorr as tc\n",
    "import math\n",
    "import scipy.spatial.distance as sd\n",
    "from timecorr.helpers import isfc, autofc, mean_combine, corrmean_combine, vec2mat, reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/dartfs/rc/lab/D/DBIC/CDL/f002s72/timecorr_paper/pieman/results'\n",
    "corrs_dir = os.path.join(data_dir, 'laplace_50_corrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For sliced correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [1,2, 3, 4]\n",
    "groups = [1, 2]\n",
    "conditions = ['intact', 'paragraph', 'word', 'rest']\n",
    "\n",
    "n = .1\n",
    "percentages = np.arange(n, 1 + n, n)\n",
    "for l in levels:\n",
    "    for c in conditions:\n",
    "        for g in groups:\n",
    "            con = os.path.join(corrs_dir, f'lev_{l}'+ f'_{c}'+ f'_{g}.npy')\n",
    "            try:\n",
    "                corrs = np.load(con)\n",
    "                mat_corrs = tc.helpers.vec2mat(corrs)\n",
    "                sliced_corrs = np.array([])\n",
    "                next_corrdir = os.path.join(data_dir, 'sliced_corrs', f'level_{l}', f'group {g}')\n",
    "                if not os.path.exists(next_corrdir):\n",
    "                    os.makedirs(next_corrdir)\n",
    "                for p in percentages:\n",
    "                    s = int(p*np.shape(mat_corrs)[-1])\n",
    "                    slice_corr = mat_corrs[:, :, s-1]\n",
    "                    if sliced_corrs.shape[0] == 0:\n",
    "                        sliced_corrs = slice_corr\n",
    "                    else:\n",
    "                        sliced_corrs = np.dstack((sliced_corrs, slice_corr))\n",
    "                np.save(os.path.join(next_corrdir, f'{c}.npy'), sliced_corrs)\n",
    "            except:\n",
    "                print('issue loading: ' + con)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For averaged correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [1,2, 3, 4]\n",
    "groups = [1, 2]\n",
    "conditions = ['intact', 'paragraph', 'word', 'rest']\n",
    "\n",
    "\n",
    "for l in levels:\n",
    "    for c in conditions:\n",
    "        for g in groups:\n",
    "            con = os.path.join(corrs_dir, f'lev_{l}'+ f'_{c}'+ f'_{g}.npy')\n",
    "            try:\n",
    "                corrs = np.load(con)\n",
    "                mat_corrs = tc.helpers.vec2mat(corrs)\n",
    "\n",
    "                next_corrdir = os.path.join(data_dir, 'mean_corrs', f'level_{l}', f'group {g}')\n",
    "                if not os.path.exists(next_corrdir):\n",
    "                    os.makedirs(next_corrdir)\n",
    "\n",
    "                mean_corrs = mat_corrs.mean(axis=2)\n",
    "\n",
    "                np.save(os.path.join(next_corrdir, f'{c}.npy'), mean_corrs)\n",
    "            \n",
    "            except:\n",
    "                print('issue loading: ' + con)\n",
    "                pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
