{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timecorr as tc\n",
    "import hypertools as hyp\n",
    "import supereeg as se\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation as ani\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob as glob\n",
    "from scipy.io import loadmat as load\n",
    "import numba\n",
    "import copy\n",
    "import nilearn as nl\n",
    "import nibabel as nib\n",
    "from nilearn import plotting as ni_plt\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "from collections import deque\n",
    "from IPython.display import HTML\n",
    "import matlab.engine as mlab\n",
    "import matlab\n",
    "from neurosynth import Dataset\n",
    "from neurosynth import meta, decode, network\n",
    "import supereeg as se\n",
    "\n",
    "\n",
    "from visbrain.objects import BrainObj, ColorbarObj, SceneObj, SourceObj\n",
    "from visbrain.io import download_file, read_stc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_indices(ary, n):\n",
    "    \"\"\"Returns the n largest indices from a numpy array.\"\"\"\n",
    "    flat = ary.flatten()\n",
    "    indices = np.argpartition(flat, -n)[-n:]\n",
    "    indices = indices[np.argsort(-flat[indices])]\n",
    "    return np.unravel_index(indices, ary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smallest_indices(ary, n):\n",
    "    \"\"\"Returns the n largest indices from a numpy array.\"\"\"\n",
    "    flat = ary.flatten()\n",
    "    indices = np.argpartition(flat, n)[:n]\n",
    "    indices = indices[np.argsort(flat[indices])]\n",
    "    return np.unravel_index(indices, ary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(centers, widths, locs):\n",
    "    \"\"\"\n",
    "    Radial basis function\n",
    "    Parameters\n",
    "    ----------\n",
    "    centers : ndarray\n",
    "        rbf coordinates (one row per RBF)\n",
    "    widths : ndarray\n",
    "        RBF radii\n",
    "    locs : ndarray\n",
    "        locations to evaluate the RBFs (one row per location)\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    results : ndarray\n",
    "        Matrix of RBF weights for each RBF (row), at each location (column)\n",
    "    \"\"\"    \n",
    "    weights = np.exp(np.divide(-cdist(locs, centers, metric='euclidean') ** 2, np.tile(np.array(widths, ndmin=2), [locs.shape[0], 1])))\n",
    "    return weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "figdir = '../figs'\n",
    "if not os.path.exists(figdir):\n",
    "    os.mkdir(figdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurosynth_dir ='../figs/neurosynth_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_f_dir = os.path.join(neurosynth_dir, 'figs')\n",
    "if not os.path.exists(n_f_dir):\n",
    "    os.mkdir(n_f_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_dir = os.path.join(neurosynth_dir, 'niis')\n",
    "if not os.path.exists(nii_dir):\n",
    "    os.mkdir(nii_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dir = os.path.join(neurosynth_dir, 'txts')\n",
    "if not os.path.exists(txt_dir):\n",
    "    os.mkdir(txt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_txt_dir = os.path.join(txt_dir, 'parsed_txts')\n",
    "if not os.path.exists(p_txt_dir):\n",
    "    os.mkdir(p_txt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.join(os.getenv('HOME'), 'Desktop', 'timecorr_env', 'timecorr_paper', 'pieman', 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = load(os.path.join(results_dir, '../data/pieman_posterior_K700.mat'))\n",
    "centers = posterior['posterior']['centers'][0][0][0][0][0]\n",
    "widths = np.array(list(posterior['posterior']['widths'][0][0][0][0][0][:, 0].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(results_dir, 'mean_corrs')\n",
    "corrs_dir = os.path.join(data_dir, 'corrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = np.arange(0,16,1)\n",
    "conditions = ['intact', 'paragraph', 'rest', 'word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = se.helpers._gray(res=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try visbrain for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "\n",
    "for l in [0,1,2,3,14]:\n",
    "    for c in conditions:\n",
    "        conds = glob.glob(os.path.join(data_dir, f'level_{l}', f'{c}.npy'))\n",
    "        g_m = np.load(conds[0])\n",
    "\n",
    "        networks = copy.copy(g_m)\n",
    "        np.fill_diagonal(networks, 0)\n",
    "        net_inds = largest_indices(np.triu(networks), top_n)\n",
    "        pos_mask = networks[net_inds] > 0\n",
    "        net_inds = np.concatenate((net_inds[0][pos_mask], net_inds[1][pos_mask]))\n",
    "        temp_locs = centers[net_inds]\n",
    "        temp_widths = widths[net_inds]\n",
    "        \n",
    "        w = rbf(temp_locs, temp_widths, template.get_locs().values)\n",
    "        b = se.Brain(data=np.array(np.sum(w, axis=0), ndmin=2), locs=template.get_locs(), minimum_voxel_size=2)\n",
    "        nii = se.Nifti(b)\n",
    "        outfile = c+ '_' + str(l+1)\n",
    "        nii.save(os.path.join(nii_dir, outfile + '_largest'))\n",
    "        ni_plt.plot_glass_brain(nii, display_mode='lyrz', output_file=os.path.join(n_f_dir, outfile + '_largest.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "\n",
    "for l in [0,1,2,3,14]:\n",
    "    for c in conditions:\n",
    "        conds = glob.glob(os.path.join(data_dir, f'level_{l}', f'{c}.npy'))\n",
    "        g_m = np.load(conds[0])\n",
    "\n",
    "        networks = copy.copy(g_m)\n",
    "        np.fill_diagonal(networks, 0)\n",
    "        net_inds = smallest_indices(np.triu(networks), top_n)\n",
    "        neg_mask = networks[net_inds] < 0\n",
    "        net_inds = np.concatenate((net_inds[0][neg_mask], net_inds[1][neg_mask]))\n",
    "        temp_locs = centers[net_inds]\n",
    "        temp_widths = widths[net_inds]\n",
    "        \n",
    "        w = rbf(temp_locs, temp_widths, template.get_locs().values)\n",
    "        b = se.Brain(data=np.array(np.sum(w, axis=0), ndmin=2), locs=template.get_locs(), minimum_voxel_size=2)\n",
    "        nii = se.Nifti(b)\n",
    "        outfile = c+ '_' + str(l+1)\n",
    "        nii.save(os.path.join(nii_dir, outfile + '_smallest'))\n",
    "        ni_plt.plot_glass_brain(nii, display_mode='lyrz', output_file=os.path.join(n_f_dir, outfile + '_smallest.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "\n",
    "for l in np.arange(0, 15, 1):\n",
    "#for l in [0,1,2,3,14]:\n",
    "    for c in conditions:\n",
    "        conds = glob.glob(os.path.join(data_dir, f'level_{l}', f'{c}.npy'))\n",
    "        g_m = np.load(conds[0])\n",
    "\n",
    "        networks = copy.copy(g_m)\n",
    "        np.fill_diagonal(networks, 0)\n",
    "        net_inds = largest_indices(np.triu(np.abs(networks)), top_n)\n",
    "        net_inds = np.concatenate((net_inds[0], net_inds[1]))\n",
    "        temp_locs = centers[net_inds]\n",
    "        temp_widths = widths[net_inds]\n",
    "        \n",
    "        w = rbf(temp_locs, temp_widths, template.get_locs().values)\n",
    "        b = se.Brain(data=np.array(np.sum(w, axis=0), ndmin=2), locs=template.get_locs(), minimum_voxel_size=2)\n",
    "        nii = se.Nifti(b)\n",
    "        outfile = c+ '_' + str(l+1)\n",
    "        nii.save(os.path.join(nii_dir, outfile + '_largest_abs'))\n",
    "        ni_plt.plot_glass_brain(nii, display_mode='lyrz', output_file=os.path.join(n_f_dir, outfile + '_largest_abs.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load(os.path.join(neurosynth_dir, 'dataset.pkl'))\n",
    "decoder = decode.Decoder(dataset=dataset, threshold=0.1, features=['taste', 'disgust', 'emotion', 'auditory', 'pain', 'somatosensory', 'conflict', 'switching', 'inhibition'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurosynth as ns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timecorr as tc\n",
    "import hypertools as hyp\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation as ani\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob as glob\n",
    "from scipy.io import loadmat as load\n",
    "import numba\n",
    "import copy\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "from collections import deque\n",
    "from neurosynth import Dataset\n",
    "from neurosynth import meta, decode, network\n",
    "# import mkl\n",
    "# mkl.set_num_threads(4)\n",
    "\n",
    "#neurosynth_dir ='/dartfs/rc/lab/D/DBIC/CDL/f002s72/timecorr_paper/pieman/results/neurosynth_data'\n",
    "nii_dir = os.path.join(neurosynth_dir, 'niis')\n",
    "txt_dir = os.path.join(neurosynth_dir, 'txts')\n",
    "\n",
    "# ns.dataset.download(path='neurosynth_dir', unpack=True)\n",
    "# dataset = Dataset(os.path.join(neurosynth_dir,'database.txt'))\n",
    "# dataset.add_features(os.path.join(neurosynth_dir, 'features.txt'))\n",
    "# dataset.save(os.path.join(neurosynth_dir,'dataset.pkl'))\n",
    "\n",
    "# dataset = Dataset.load(os.path.join(neurosynth_dir, 'dataset.pkl'))\n",
    "# decoder = decode.Decoder(dataset=dataset, threshold=0.1, features=None)\n",
    "\n",
    "conditions = ['intact', 'paragraph', 'rest', 'word']\n",
    "\n",
    "for l in [1,2,3,4,15]:\n",
    "    for c in conditions:\n",
    "        in_file = c+ '_' + str(l)\n",
    "        data = decoder.decode([os.path.join(nii_dir, in_file + '_smallest.nii')], save=None)\n",
    "        renamed = data.reset_index()\n",
    "        renamed.columns = ['feature', 'value']\n",
    "        s_rename = renamed.sort_values(by=['value'], ascending=False)\n",
    "        s_rename.to_csv(os.path.join(txt_dir, in_file +'_smallest.txt'))\n",
    "        \n",
    "\n",
    "for l in [1,2,3,4,15]:\n",
    "    for c in conditions:\n",
    "        in_file = c+ '_' + str(l)\n",
    "        data = decoder.decode([os.path.join(nii_dir, in_file + '_largest.nii')], save=None)\n",
    "        renamed = data.reset_index()\n",
    "        renamed.columns = ['feature', 'value']\n",
    "        s_rename = renamed.sort_values(by=['value'], ascending=False)\n",
    "        s_rename.to_csv(os.path.join(txt_dir, in_file +'_largest.txt'))\n",
    "        \n",
    "for l in [1,2,3,4,15]:\n",
    "    for c in conditions:\n",
    "        in_file = c+ '_' + str(l)\n",
    "        data = decoder.decode([os.path.join(nii_dir, in_file + '_largest_abs.nii')], save=None)\n",
    "        renamed = data.reset_index()\n",
    "        renamed.columns = ['feature', 'value']\n",
    "        s_rename = renamed.sort_values(by=['value'], ascending=False)\n",
    "        s_rename.to_csv(os.path.join(txt_dir, in_file +'_largest_abs.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['intact', 'paragraph', 'rest', 'word']\n",
    "full_pd = pd.DataFrame()\n",
    "for l in [1,2,3,4]:\n",
    "    for c in conditions:\n",
    "        in_file = c+ '_' + str(l)\n",
    "        temp_file = pd.read_csv(os.path.join(txt_dir, in_file +'_smallest.txt'))\n",
    "        temp_file['condition'] = c\n",
    "        temp_file['order'] = l\n",
    "        if full_pd.empty:\n",
    "                full_pd = temp_file[:10]\n",
    "        else:\n",
    "            full_pd = full_pd.append(temp_file[:10])\n",
    "\n",
    "pretty_pd = full_pd[['order','condition', 'value', 'feature']]\n",
    "pretty_pd.to_csv(os.path.join(txt_dir,'neg_10_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['intact', 'paragraph', 'rest', 'word']\n",
    "full_pd = pd.DataFrame()\n",
    "for l in [1,2,3,4]:\n",
    "    for c in conditions:\n",
    "        in_file = c+ '_' + str(l)\n",
    "        temp_file = pd.read_csv(os.path.join(txt_dir, in_file +'_largest_abs.txt'))\n",
    "        temp_file['condition'] = c\n",
    "        temp_file['order'] = l\n",
    "        if full_pd.empty:\n",
    "                full_pd = temp_file[:10]\n",
    "        else:\n",
    "            full_pd = full_pd.append(temp_file[:10])\n",
    "\n",
    "pretty_pd = full_pd[['order','condition', 'value', 'feature']]\n",
    "pretty_pd.to_csv(os.path.join(txt_dir,'abs_10_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "conditions = ['intact', 'paragraph', 'rest', 'word']\n",
    "full_pd = pd.DataFrame()\n",
    "for l in [1,2,3,4]:\n",
    "    for c in conditions:\n",
    "        in_file = c+ '_' + str(l)\n",
    "        temp_file = pd.read_csv(os.path.join(txt_dir, in_file +'_largest.txt'))\n",
    "        temp_file['condition'] = c\n",
    "        temp_file['order'] = l\n",
    "\n",
    "        if full_pd.empty:\n",
    "                full_pd = temp_file[:10]\n",
    "        else:\n",
    "            full_pd = full_pd.append(temp_file[:10])\n",
    "\n",
    "pretty_pd = full_pd[['order','condition', 'value', 'feature']]\n",
    "pretty_pd.to_csv(os.path.join(txt_dir,'pos_10_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_pd = full_pd[['order','condition', 'value', 'feature']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_pd.to_csv(os.path.join(txt_dir,'negative_10_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### DO THIS ON THE CLUSTER for coactivateion maps\n",
    "### only do this once\n",
    "# import neurosynth as ns\n",
    "# ns.dataset.download(path='neurosynth_dir', unpack=True)\n",
    "# dataset = Dataset(os.path.join(neurosynth_dir,'database.txt'))\n",
    "# dataset.add_features(os.path.join(neurosynth_dir, 'features.txt'))\n",
    "# dataset.save(os.path.join(neurosynth_dir,'dataset.pkl'))\n",
    "\n",
    "# dataset = Dataset.load(os.path.join(neurosynth_dir, 'dataset.pkl'))\n",
    "# decoder = decode.Decoder(dataset=dataset, threshold=0.1, features=['taste', 'disgust', 'emotion', 'auditory', 'pain', 'somatosensory', 'conflict', 'switching', 'inhibition'])\n",
    "\n",
    "# for e, label in enumerate(cond_labels):\n",
    "#     order = label[0]\n",
    "#     con = label[1]\n",
    "#     top_10 = top_coors[:, :, e]\n",
    "\n",
    "#     network.coactivation(dataset, top_10.tolist(), threshold=0.1, r=10, output_dir=neurosynth_dir, prefix=con+ '_' + str(order))\n",
    "    \n",
    "# for e, label in enumerate(cond_labels):\n",
    "#     list_files = glob.glob(os.path.join(neurosynth_dir, con+ '_' + str(order + '*')))\n",
    "#     decoder = decode.Decoder(dataset=dataset, threshold=0.1, features=['taste', 'disgust', 'emotion', 'auditory', 'pain', 'somatosensory', 'conflict', 'switching', 'inhibition'])  # can also pass other useful args\n",
    "#     results_df = decoder.decode(images=list_files, save=None)  # can also pass names specific for the images\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
